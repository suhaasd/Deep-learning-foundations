{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7g7bxFCHxGP"
   },
   "source": [
    "$$\\Large\\boxed{\\text{AME 5202 Deep Learning, Even Semester 2026}}$$\n",
    "\n",
    "$$\\large\\text{Theme}: \\underline{\\text{linear algebra computational foundation for deep learning}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDK9fC6uiBGE"
   },
   "source": [
    "---\n",
    "\n",
    "Load essential libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1759082932391,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "20W0d4ruQjE4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfYXkqmLiVLM"
   },
   "source": [
    "---\n",
    "\n",
    "Mount Google Drive folder if running Google Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYzBBBxqiaGa"
   },
   "outputs": [],
   "source": [
    "## Mount Google drive folder if running in Colab\n",
    "if('google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/EvenSem2026MAHE'\n",
    "    DATA_DIR = DIR+'/Data/'\n",
    "else:\n",
    "    DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avVZ6D1ZgEUT"
   },
   "source": [
    "---\n",
    "\n",
    "**We will now use PyTorch to create tensors**\n",
    "\n",
    "The patient data matrix:\n",
    "\n",
    "![patient data matrix](https://1drv.ms/i/s!AjTcbXuSD3I3hsxIkL4V93-CGq8RkQ?embed=1&width=1000)\n",
    "\n",
    "**Notation**:\n",
    "\n",
    "Zeroth patient vector $\\mathbf{x}^{(0)}= \\begin{bmatrix}72\\\\120\\\\37.3\\\\104\\\\32.5\\end{bmatrix}$ and zeroth feature (heart rate vector) $\\mathbf{x}_0 = \\begin{bmatrix}72\\\\85\\\\68\\\\90\\\\84\\\\78\\end{bmatrix}.$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrPnepAEvr0O"
   },
   "outputs": [],
   "source": [
    "## Create a patient data matrix as a constant tensor\n",
    "X = torch.tensor([\n",
    "                  [72, 120, 37.3, 104, 32.5],\n",
    "                  [85, 130, 37.0, 110, 14],\n",
    "                  [68, 110, 38.5, 125, 34],\n",
    "                  [90, 140, 38.0, 130, 26],\n",
    "                  [84, 132, 38.3, 146, 30],\n",
    "                  [78, 128, 37.2, 102, 12]\n",
    "                  ],\n",
    "                  dtype = torch.float64)\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "print(X[0]) # this is patient-0 info which is a rank-1 tensor or a vector\n",
    "print(X[0, :]) # this is also patient-0 info\n",
    "print(X[0, 2]) # feature-2 (temperature) of patient-0\n",
    "print(X[-1]) # this is the last patient (patient-5) info\n",
    "print(X[:, 1]) # feature-1 (BP) of all the patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cevtn_b4gek5"
   },
   "source": [
    "---\n",
    "\n",
    "**Convert a PyTorch object into a numpy array**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QS3MmzwsgkWU"
   },
   "source": [
    "---\n",
    "\n",
    "**Addition and subtraction of vectors, scalar multiplication (apply operation componentwise)**\n",
    "\n",
    "![vector addition](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NokBAAAAAZLAaAoWwhtn8Vk26NotALo?width=256)\n",
    "\n",
    "![vector subtracton](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3M4kBAAAAAU_n_mAEv006QFZm_sUj2Dc?width=256)\n",
    "\n",
    "![vector multiplication](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3NIkBAAAAAa_qL04bLT4kWoNeHcrR9LQ?width=256)\n",
    "\n",
    "![vector geometry1](https://1drv.ms/i/c/37720f927b6ddc34/IQSGNMr5z3SSRry7LSKL7LybAcGYuzgw5smabV8-6DudXIs?width=230)\n",
    "\n",
    "![vector geometry2](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=192)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgPtJP0sglQP"
   },
   "outputs": [],
   "source": [
    "# Vector addition\n",
    "print(X[1, :] + X[2, :])\n",
    "\n",
    "# Vector subtraction\n",
    "print(X[1, :] - X[2, :])\n",
    "\n",
    "# Scalar-vector multiplication\n",
    "(9/5) * X[:, 2] + 32 # Temperature in Fahrenheit\n",
    "\n",
    "# Average patient\n",
    "x_avg = (1/6)*(X[0, :] + X[1, :] + X[2, :] + X[3, :] + X[4, :] + X[5, :])\n",
    "print(x_avg)\n",
    "\n",
    "# Using the in-built torch function\n",
    "x_avg = torch.mean(X, dim = 0)\n",
    "print(x_avg)\n",
    "\n",
    "# Another broadcasting example\n",
    "print(X - x_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t_qXrlCROKA"
   },
   "source": [
    "---\n",
    "\n",
    "Application of vector subtraction in natural language processing (NLP): download the word embedding model trained on Wikipedia articles.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2382,
     "status": "ok",
     "timestamp": 1759082990681,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "e3_SM-1-gwoe"
   },
   "outputs": [],
   "source": [
    "# Load the Wikipedia-trained GLoVe word vectors (50-dimensional) from the pickle file\n",
    "with open(DATA_DIR + 'glove_wiki_gigaword_50.pkl', 'rb') as f:\n",
    "    loaded_word_vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YRVJferRlK5"
   },
   "source": [
    "---\n",
    "\n",
    "Now we will see what embedding vector comes as a result of applying the model for the words *cricket* and *football*.\n",
    "\n",
    "Next, we will do an *intuitive* subtraction of word embeddings as in\n",
    "\n",
    "1. cricket without tendulkar\n",
    "2. football without messi\n",
    "\n",
    "Note that the embedding vectors have 50 components corresponding to the 50-dimensional embedding of model suggested by the name '**glove-wiki-gigaword-50**'\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1759083083040,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "HVVFzeQyR3Wb",
    "outputId": "66ffb370-e529-431e-a88e-7dcf6df119df"
   },
   "outputs": [],
   "source": [
    "# Cricket without Tendulkar\n",
    "a = loaded_word_vectors.get('cricket', None) - loaded_word_vectors.get('tendulkar', None)\n",
    "\n",
    "# Football without Messi\n",
    "b = loaded_word_vectors.get('football', None) - loaded_word_vectors.get('messi', None)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# How different is cricket-without-tendulkar from\n",
    "# football-without-messi?\n",
    "print(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6nbdX9IAYu6"
   },
   "source": [
    "---\n",
    "\n",
    "Understanding pen & paper versions of tensors w.r.t. their representations in the code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhYNdr8DAj2V"
   },
   "outputs": [],
   "source": [
    "# Pen & paper: 3-vector, Code: rank-1 tensor\n",
    "a_vector = torch.tensor([1.0, 2.0, 3.0], dtype = torch.float64)\n",
    "print(a_vector)\n",
    "print(a_vector.shape)\n",
    "print('-------')\n",
    "# Pen & paper: 1x3-matrix, Code: rank-2 tensor\n",
    "a_matrix_version1 = torch.tensor([[1.0, 2.0, 3.0]], dtype = torch.float64)\n",
    "print(a_matrix_version1)\n",
    "print(a_matrix_version1.shape)\n",
    "print('-------')\n",
    "# Pen & paper: 3x1-matrix, Code: rank-2 tensor\n",
    "a_matrix_version2 = torch.tensor([[1.0], [2.0], [3.0]], dtype = torch.float64)\n",
    "print(a_matrix_version2)\n",
    "print(a_matrix_version2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VPICS8ggvvg"
   },
   "source": [
    "---\n",
    "\n",
    "A tensor of rank 3 corresponding to 4 time stamps (hourly), 3 samples (patients), 2 features (HR and BP). Assume that admission time is 9AM.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1758046306908,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "yQAvgkRkWAM8",
    "outputId": "a150fad4-7cd6-48ca-9edd-4e40468d8f55"
   },
   "outputs": [],
   "source": [
    "# A rank-3 patient tensor with shape (4, 3, 2)\n",
    "# with meaning for\n",
    "# dim-0 as 4 hourly timestamps,\n",
    "# dim-1 as 3 patients, and\n",
    "# dim-2 as 2 features (HR and BP)\n",
    "# T = torch.tensor([[[HR, BP], [HR, BP], [HR, BP]],\n",
    "#                   [[HR, BP], [HR, BP], [HR, BP]],\n",
    "#                   [[HR, BP], [HR, BP], [HR, BP]],\n",
    "#                   [[HR, BP], [HR, BP], [HR, BP]]])\n",
    "T = torch.tensor([[[74., 128], [79, 116], [71, 116]],\n",
    "                 [[78, 118], [82, 124], [72, 128]],\n",
    "                 [[84, 138], [84, 130], [74, 120]],\n",
    "                 [[82, 126], [76, 156], [82, 132]]])\n",
    "print(T.shape)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JV0fpSojg2EZ"
   },
   "source": [
    "---\n",
    "\n",
    "**Accessing elements of a tensor**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GbZuDYqg22n"
   },
   "outputs": [],
   "source": [
    "## Accessing elements of a tensor\n",
    "# Rank-3 tensor T has axes order (timestamps, patients, features)\n",
    "\n",
    "# Element of T at postion 3 w.r.t. dim-0, position 2 w.r.t. dim-1,\n",
    "# position-1 w.r.t dim-2\n",
    "? # timestamp-3, patient-2, feature -1 also the BP of patient-2 at noon\n",
    "\n",
    "# Element-0 of object T which is also the info for all patients at\n",
    "# admission time 9AM\n",
    "? # patient's info at admission time\n",
    "\n",
    "# Last admitted patient's info at noon\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Understanding shapes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = torch.tensor([1.0, 2.0, 3.0]) # rank-1 tensor, a 3-vector in pen & paper\n",
    "#a = torch.tensor([[1.0, 2.0, 3.0]]) # rank-2 tensor, a 1x3-matrix in pen & paper\n",
    "#a = torch.tensor([[1.0], [2.0], [3.0]]) # rank-2 tensor, a 3x1-matrix in pen & paper\n",
    "a = torch.tensor([[[1.0, 2.0, 3.0]]]) # rank-3 tensor, a 1x1x3-tensor in pen & paper\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Broadcasting example that does not work\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How different are the patients compared to patient-0 across all timestamps\n",
    "T_patient0 = T[:, 0, :]\n",
    "print(T)\n",
    "print(T_patient0)\n",
    "print(T.shape)\n",
    "print(T_patient0.shape)\n",
    "#print(T - T_patient0) # does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Broadcasting example that works\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_patient0 = T[:, 0, :]\n",
    "# Add a new dimension to a tensor using the unsqueeze() function\n",
    "T_patient0_new = torch.unsqueeze(T_patient0, 1)\n",
    "print(T_patient0_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise**: interpret $\\texttt{T[:, -1, :]}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret T[:, -1, :]\n",
    "T[:, -1, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "$l_2$ norm or the geometric length of a vector denoted as $\\lVert \\mathbf{a}\\rVert_2$ tells us how long a vector is. In 2-dimensions, $$\\mathbf{a}=\\begin{bmatrix}a_1\\\\a_2\\end{bmatrix}\\Rightarrow \\lVert\\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2}$$ and in $n$-dimensions, $$\\mathbf{a}=\\begin{bmatrix}a_1\\\\a_2\\\\\\vdots\\\\a_n\\end{bmatrix}\\Rightarrow\\lVert \\mathbf{a}\\rVert_2 = \\sqrt{a_1^2+a_2^2+\\cdots+a_n^2}.$$\n",
    "\n",
    "![vector norm](https://1drv.ms/i/c/37720f927b6ddc34/IQT817WmpQjlRqZ1R0d5Cfv6AUW6c4robL-gk06i9wmCaFU?width=500)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## l2 norm of a vector\n",
    "x = torch.tensor([76.0, 124.0], dtype = torch.float64)\n",
    "print(x)\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Application of norm: how different is 'cricket-without-tendulkar' compared to 'football-without-messi'?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to 'cricket-without-tendulkar' and 'football-without-messi'\n",
    "print(torch.norm(torch.tensor(a))) # norm of 'cricket-without-tendulkar'\n",
    "print(torch.norm(torch.tensor(b))) # norm of 'football-without-messi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Dot Product of Vectors**\n",
    "\n",
    "A scalar resulting from an elementwise multiplication and addition: $$\\mathbf{a}{\\color{cyan}\\cdot}\\mathbf{b} = {\\color{red}{a_1b_1}}+{\\color{green}{a_2b_2}}+\\cdots+{\\color{magenta}{a_nb_n}}$$\n",
    "\n",
    "The <font color=\"cyan\">dot</font> ${\\color{cyan}\\cdot}$ represents the computation of the dot product.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dot product of vectors\n",
    "a = torch.tensor([1.0, 2.0, 3.0], dtype = torch.float64)\n",
    "b = torch.tensor([4.0, 5.0, 6.0], dtype = torch.float64)\n",
    "print(a * b) # This is called the Hadamard product\n",
    "print(torch.dot(a, b)) # This is called the dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The dot product is a measure of similarity between vectors (or, how aligned they are geometrically).\n",
    "\n",
    "![dot product](https://1drv.ms/i/c/37720f927b6ddc34/IQTbcGSjdbhSTJ7J39d5BCWAAWS6-y5U6J87vHuDWeAqGwM?width=6000)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0])\n",
    "b = torch.tensor([2.0, 4.0])\n",
    "c = torch.tensor([-2.0, 1.0])\n",
    "d = torch.tensor([-1.0, -2.0])\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Cauchy-Schwarz inequality: for any two vectors $\\mathbf{x}$ and $\\mathbf{y},$ it is always true that $$-1\\leq\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\leq1.$$\n",
    "\n",
    "This is a normalized measure of similarity (or extent of alignment) between vectors which also referred to as the cosine similarity.\n",
    "\n",
    "This helps define the angle between two vectors $\\mathbf{x}$ and $\\mathbf{y}$ as $$\\angle(\\mathbf{x},\\mathbf{y}) = \\cos^{-1}\\left(\\frac{\\mathbf{x}\\cdot{\\mathbf{y}}}{\\lVert\\mathbf{x}\\rVert_2\\lVert\\mathbf{y}\\rVert_2}\\right)$$ which is a value from $0$ through $\\pi$ radians.\n",
    "\n",
    "Two ways to measure the difference between two vectors:\n",
    "\n",
    "![angle](https://1drv.ms/i/c/37720f927b6ddc34/IQQ03G17kg9yIIA3WokBAAAAAQi8FPV9YCebl5WnyEKJ3vg?width=213&height=400)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0])\n",
    "y = torch.tensor([2.0, 1.0])\n",
    "\n",
    "# Linear difference between x and y\n",
    "?\n",
    "\n",
    "# Angle difference between x and y in radians\n",
    "?\n",
    "\n",
    "# Angle difference between x and y in degrees\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Application of the Cauchy-Schwarz inequality: how different is \"cricket without tendulkar\" from \"football without messi\"?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(loaded_word_vectors.get('cricket', None) - loaded_word_vectors.get('tendulkar', None),\n",
    "                 dtype = torch.float64)\n",
    "b = torch.tensor(loaded_word_vectors.get('football', None) - loaded_word_vectors.get('messi', None),\n",
    "                 dtype = torch.float64)\n",
    "\n",
    "# Linear difference between and a and b\n",
    "print(torch.norm(a-b))\n",
    "\n",
    "# Angle difference between a and b in radians (cosine similarity)\n",
    "print(torch.acos(torch.dot(a, b) / (torch.norm(a) * torch.norm(b))))\n",
    "\n",
    "# Angle difference between a and b in degrees\n",
    "print((180.0/torch.pi)*(torch.acos(torch.dot(a, b) / (torch.norm(a) * torch.norm(b)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Hadamard Product of Vectors**\n",
    "\n",
    "A vector resulting from an elementwise multiplication: $$\\mathbf{a}{\\color{cyan}\\otimes}\\mathbf{b} = \\begin{bmatrix}{\\color{red}{a_1\\times b_1}}\\\\{\\color{green}{a_2\\times b_2}}\\\\\\vdots\\\\{\\color{magenta}{a_n\\times b_n}}\\end{bmatrix}.$$\n",
    "\n",
    "The $\\color{cyan}\\otimes$ represents the computation of the Hadamard product.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4., 10., 18.], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hadamard product\n",
    "a = torch.tensor([1.0, 2.0, 3.0], dtype = torch.float64)\n",
    "b = torch.tensor([4.0, 5.0, 6.0], dtype = torch.float64)\n",
    "\n",
    "# Element-wise multiplication (Hadamard product)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "A matrix-vector product is simply a sequence of dot products of the rows of the matrix (seen as vectors) with the vector\n",
    "\n",
    "![matvec product](https://1drv.ms/i/c/37720f927b6ddc34/IQQ1cQ8fZdFmS4cnGkBlsZbAAaL2zMtzWdjHe-HCMt4UTA0?width=700)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix-vector product\n",
    "A = torch.tensor([[1.0, 2.0, 4.0],\n",
    "                  [2.0, -1.0, 3.0]])\n",
    "x = torch.tensor([4.0, 2.0, -2.0])\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "A matrix-matrix product is simply a sequence of matrix-vector products.\n",
    "\n",
    "![matmatprod](https://1drv.ms/i/c/37720f927b6ddc34/IQQ-B3z7tbWHQqBrW9k2ElDVAUc5fWzM24txLkgBK7f8Yac?width=550)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 11.],\n",
       "        [ 0.,  7.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Matrix-matrix product\n",
    "A = torch.tensor([[1.0, 2.0, 4.0],\n",
    "                  [2.0, -1.0, 3.0]], dtype = torch.float64)\n",
    "\n",
    "B = torch.tensor([[4, -1],\n",
    "                  [2, 0],\n",
    "                  [-2, 3]], dtype = torch.float64)\n",
    "torch.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Matrix-matrix product using patient data matrix and a weights matrix:\n",
    "\n",
    "![patient dataset](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=800)\n",
    "\n",
    "$$\\mathbf{Z} = \\mathbf{XW}.$$\n",
    "\n",
    "$\\mathbf{Z}$ is called the raw scores matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient data matrix X:\n",
      " tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n",
      "        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n",
      "        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n",
      "        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n",
      "        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n",
      "        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]],\n",
      "       dtype=torch.float64)\n",
      "Weights matrix:\n",
      " tensor([[-0.1000,  0.5000,  0.3000],\n",
      "        [ 0.9000,  0.3000,  0.5000],\n",
      "        [-1.5000,  0.4000,  0.1000],\n",
      "        [ 0.1000,  0.1000, -1.0000],\n",
      "        [-1.2000,  0.5000, -0.8000]], dtype=torch.float64)\n",
      "tensor([[ 16.2500, 113.5700, -44.6700],\n",
      "        [ 47.2000, 114.3000, -27.0000],\n",
      "        [  6.1500, 111.9000, -72.9500],\n",
      "        [ 41.8000, 128.2000, -50.0000],\n",
      "        [ 31.5500, 126.5200, -74.9700],\n",
      "        [ 47.4000, 108.4800, -20.4800]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Patients data matrix\n",
    "X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n",
    "                 [85, 130, 37.0, 110, 14],\n",
    "                 [68, 110, 38.5, 125, 34],\n",
    "                 [90, 140, 38.0, 130, 26],\n",
    "                 [84, 132, 38.3, 146, 30],\n",
    "                 [78, 128, 37.2, 102, 12]], dtype = torch.float64)\n",
    "print(f'Patient data matrix X:\\n {X}') #f-string in Python\n",
    "\n",
    "# Weights matrix\n",
    "W = torch.tensor([[-0.1, 0.5, 0.3],\n",
    "                  [0.9, 0.3, 0.5],\n",
    "                  [-1.5, 0.4, 0.1],\n",
    "                  [0.1, 0.1, -1.0],\n",
    "                  [-1.2, 0.5, -0.8]], dtype = torch.float64)\n",
    "print(f'Weights matrix:\\n {W}')\n",
    "\n",
    "# Raw scores matrix (matrix-matrix multiplication)\n",
    "Z = torch.matmul(X, W)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Version-1** view of the matrix-matrix product $\\mathbf{Z} = \\mathbf{XW}$:\n",
    "\n",
    "*What a particular neuron understands about a particular patient.*\n",
    "\n",
    "![matrix-matrix product version-1](https://1drv.ms/i/c/37720f927b6ddc34/IQQdAOCwtndURKA-h4yvpTqlAYjBjlcweRSeMYkPvf7dwmQ?width=660)\n",
    "\n",
    "$$\\begin{align*}[\\mathbf{Z}]_{i,j} &= (i,j)\\text{-th element of }\\mathbf{Z}\\\\&=\\text{what the }j\\text{th neuron learns about the } i\\text{th patient}\\\\&=\\mathbf{x}^{(i)}\\cdot\\mathbf{w}_j\\\\& = {\\mathbf{x}^{(i)}}^\\mathrm{T}\\mathbf{w}_j\\\\\\Rightarrow \\underbrace{[\\mathbf{Z}]_{{\\color{yellow}0},{\\color{cyan}2}}}_{{\\color{yellow}0}\\text{th patient},\\,{\\color{cyan}2}\\text{nd neuron}} &= \\mathbf{x}^{({\\color{yellow}0})}\\cdot\\mathbf{w}_{{\\color{cyan}2}}\\\\ &= \\begin{bmatrix}72\\\\120\\\\37.3\\\\104\\\\32.5\\end{bmatrix}\\cdot\\begin{bmatrix}0.3\\\\0.5\\\\0.1\\\\-1.0\\\\-0.8\\end{bmatrix}\\\\ &= -44.67.\\end{align*}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-44.6700, dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The (0, 2)-th element of the matrix-matrix product XW\n",
    "torch.dot(X[0, :], W[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Version-2** view of the matrix-matrix product $\\mathbf{Z} = \\mathbf{XW}$:\n",
    "\n",
    "*What a particular neuron understands about all the patients.*\n",
    "\n",
    "![matrix-matrix product version-2](https://1drv.ms/i/c/37720f927b6ddc34/IQRm1-w-6TG0R4C4J4BizyzyAWIbcHzbEjgmx-0JFREdHsE?width=660)\n",
    "\n",
    "$$\\begin{align*}\\mathbf{z}_j &= \\mathbf{X}\\mathbf{w}_j\\\\&=\\text{what the } j\\text{th neuron learns about the all the patients}\\\\&=w_{j,0}\\times\\textbf{HR}+w_{j,1}\\times\\textbf{BP}+w_{j,2}\\times\\textbf{Temp}+w_{j,3}\\times\\textbf{Sugar}+w_{j,4}\\times\\textbf{Vitamin D}\\\\&= w_{j,0}\\mathbf{x}_0+w_{j,1}\\mathbf{x}_1+w_{j,2}\\mathbf{x}_2+w_{j,3}\\mathbf{x}_3+w_{j,4}\\mathbf{x}_4\\\\\\Rightarrow\\underbrace{\\mathbf{z}_{{\\color{cyan}0}}}_{{\\color{cyan}0}\\text{th neuron understanding}} &= \\underbrace{\\mathbf{X}}_{\\color{yellow}{\\text{all patients}}}\\ \\underbrace{\\mathbf{w}_{{\\color{cyan}0}}}_{{\\color{cyan}0}\\text{th neuron weights}}\\\\&= {\\color{cyan}{-0.1}}\\times\\begin{bmatrix}{\\color{yellow}{72}}\\\\{\\color{yellow}{85}}\\\\{\\color{yellow}{68}}\\\\{\\color{yellow}{90}}\\\\{\\color{yellow}{84}}\\\\{\\color{yellow}{78}}\\end{bmatrix}+{\\color{cyan}{0.9}}\\times\\begin{bmatrix}{\\color{yellow}{120}}\\\\{\\color{yellow}{130}}\\\\{\\color{yellow}{110}}\\\\{\\color{yellow}{140}}\\\\{\\color{yellow}{132}}\\\\{\\color{yellow}{128}}\\end{bmatrix}+({\\color{cyan}{-1.5}})\\times\\begin{bmatrix}{\\color{yellow}{37.3}}\\\\{\\color{yellow}{37.0}}\\\\{\\color{yellow}{38.5}}\\\\{\\color{yellow}{38.0}}\\\\{\\color{yellow}{38.3}}\\\\{\\color{yellow}{37.2}}\\end{bmatrix}+{\\color{cyan}{0.1}}\\times\\begin{bmatrix}{\\color{yellow}{104}}\\\\{\\color{yellow}{110}}\\\\{\\color{yellow}{125}}\\\\{\\color{yellow}{130}}\\\\{\\color{yellow}{146}}\\\\{\\color{yellow}{102}}\\end{bmatrix}+({\\color{cyan}{-1.2}})\\times\\begin{bmatrix}{\\color{yellow}{32.5}}\\\\{\\color{yellow}{14}}\\\\{\\color{yellow}{34}}\\\\{\\color{yellow}{26}}\\\\{\\color{yellow}{30}}\\\\{\\color{yellow}{12}}\\end{bmatrix}\\\\&=\\begin{bmatrix}16.25\\\\47.20\\\\6.15\\\\41.80\\\\31.55\\\\47.40\\end{bmatrix}.\\end{align*}$$\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16.2500, 47.2000,  6.1500, 41.8000, 31.5500, 47.4000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The 0-th column of the matrix-matrix product XW\n",
    "torch.matmul(X, W[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Version-3** view of the matrix-matrix product $\\mathbf{Z} = \\mathbf{XW}$:\n",
    "\n",
    "*What all neurons understand about a particular patient.*\n",
    "\n",
    "![matrix-matrix product version-3](https://1drv.ms/i/c/37720f927b6ddc34/IQRfO-qEJQ9mQYLH_f-lyjeQAaWV4FrDjTjaEHJpPB1PmCg?width=660)\n",
    "\n",
    "$$\\begin{align*}{\\mathbf{z}^{(i)}}^\\mathrm{T}&={\\mathbf{x}^{(i)}}^\\mathrm{T}\\mathbf{W}\\\\&= \\text{what is learned about the }i\\text{th patient by all the neurons}\\\\&=i\\text{th HR }\\times{\\mathbf{w}^{(0)}}^\\mathrm{T}+i\\text{th BP }\\times{\\mathbf{w}^{(1)}}^\\mathrm{T}+i\\text{th Temp }\\times{\\mathbf{w}^{(2)}}^\\mathrm{T}+i\\text{th Sugar }\\times{\\mathbf{w}^{(3)}}^\\mathrm{T}+i\\text{th Vitamin D }\\times{\\mathbf{w}^{(4)}}^\\mathrm{T}\\\\&=x^{(i)}_0\\times{\\mathbf{w}^{(0)}}^\\mathrm{T}+x^{(i)}_1\\times{\\mathbf{w}^{(1)}}^\\mathrm{T}+x^{(i)}_2\\times{\\mathbf{w}^{(2)}}^\\mathrm{T}+x^{(i)}_3\\times{\\mathbf{w}^{(3)}}^\\mathrm{T}+x^{(i)}_4\\times{\\mathbf{w}^{(4)}}^\\mathrm{T}\\\\\\underbrace{\\Rightarrow{{\\mathbf{z}^{({\\color{yellow}0})}}^\\mathrm{T}}}_{{\\color{yellow}{0}}\\text{th patient understanding}}&=\\underbrace{{{\\mathbf{x}^{({\\color{yellow}0})}}^\\mathrm{T}}}_{{\\color{yellow}{0}}\\text{th patient}}\\ \\underbrace{\\mathbf{W}}_{{\\color{cyan}{\\text{all neurons}}}}\\\\ &= {\\color{yellow}{72}}\\times\\begin{bmatrix}{\\color{cyan}{-0.1}} & {\\color{cyan}{0.5}} & {\\color{cyan}{0.3}}\\end{bmatrix} \\\\&+ {\\color{yellow}{120}}\\times\\begin{bmatrix}{\\color{cyan}{0.9}} & {\\color{cyan}{0.3}} & {\\color{cyan}{0.5}}\\end{bmatrix}\\\\&+{\\color{yellow}{37.3}}\\times\\begin{bmatrix}{\\color{cyan}{-1.5}} & {\\color{cyan}{0.4}} & {\\color{cyan}{0.1}}\\end{bmatrix}\\\\&+{\\color{yellow}{104}}\\times\\begin{bmatrix}{\\color{cyan}{0.1}} & {\\color{cyan}{0.1}} & {\\color{cyan}{-1.0}}\\end{bmatrix}\\\\&+{\\color{yellow}{32.5}}\\times\\begin{bmatrix}{\\color{cyan}{-1.2}} & {\\color{cyan}{0.5}} & {\\color{cyan}{-0.8}}\\end{bmatrix}\\\\&=\\begin{bmatrix}16.25 & 113.57 & -44.67\\end{bmatrix}.\\end{align*}$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 16.2500, 113.5700, -44.6700], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The 0-th row of the matrix-matrix product XW\n",
    "torch.matmul(X[0, :], W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The softmax function: takes a $k$-vector $\\mathbf{z}$ as input and returns a vector $\\mathbf{a}$ of the same shape as the output which is referred to as the softmax-activated scores.\n",
    "\n",
    "$$\\begin{align*}\\mathbf{a}&=\\text{softmax}(\\mathbf{z})=\\begin{bmatrix}\\dfrac{e^{z_1}}{e^{z_1}+e^{z_2}+\\cdots+e^{z_k}}\\\\\\dfrac{e^{z_2}}{e^{z_1}+e^{z_2}+\\cdots+e^{z_k}}\\\\\\vdots\\\\\\dfrac{e^{z_k}}{e^{z_1}+e^{z_2}+\\cdots+e^{z_k}}\\end{bmatrix}.\\end{align*}$$\n",
    "\n",
    "In the following example, we consider a raw scores vector $\\mathbf{z}$ with 3 components which leads to the softmax-activated scores vectors $\\mathbf{a}$ which can be interpreted as the predicted probabilities that the sample belongs to each one of the output classes:\n",
    "\n",
    "![softmax](https://1drv.ms/i/s!AjTcbXuSD3I3hscmdol7J2G4GDo5WQ?embed=1&width=660)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5.6000,  6.4000, -4.5000], dtype=torch.float64)\n",
      "tensor([3.1002e-01, 6.8997e-01, 1.2736e-05], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([5.6, 6.4, -4.5], dtype = torch.float64)\n",
    "print(z)\n",
    "softmax = torch.nn.Softmax(dim = 0)\n",
    "a = softmax(z)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Calculating the raw scores followed by the softmax-activated scores for the patient data matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw zcores matrix:\n",
      " tensor([[ 16.2500, 113.5700, -44.6700],\n",
      "        [ 47.2000, 114.3000, -27.0000],\n",
      "        [  6.1500, 111.9000, -72.9500],\n",
      "        [ 41.8000, 128.2000, -50.0000],\n",
      "        [ 31.5500, 126.5200, -74.9700],\n",
      "        [ 47.4000, 108.4800, -20.4800]], dtype=torch.float64)\n",
      "tensor([[5.4258e-43, 1.0000e+00, 1.8934e-69],\n",
      "        [7.2250e-30, 1.0000e+00, 4.3071e-62],\n",
      "        [1.1840e-46, 1.0000e+00, 5.2561e-81],\n",
      "        [2.9989e-38, 1.0000e+00, 4.0618e-78],\n",
      "        [5.6892e-42, 1.0000e+00, 3.1189e-88],\n",
      "        [2.9737e-27, 1.0000e+00, 9.8488e-57]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Raw scores matrix (matrix-matrix multiplication)\n",
    "Z = torch.matmul(X, W)\n",
    "print(f'Raw zcores matrix:\\n {Z}')\n",
    "\n",
    "# Calculate the softmax scores\n",
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "A = softmax(Z)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Standardization of data to get rid of the effects of units.\n",
    "\n",
    "The standard deviation of a vector is a measure of how much the components or elements of that vector typically deviate from their average value. For an $n$-vector $\\mathbf{x},$ the standard deviation is denoted and calculated as\n",
    "$$\\mathbf{x} = \\begin{bmatrix}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{bmatrix}\\Rightarrow\\text{std}(\\mathbf{x}) = \\sqrt{\\frac{\\left[x_1-\\text{avg}(\\mathbf{x})\\right]^2+\\cdots+\\left[x_n-\\text{avg}(\\mathbf{x})\\right]^2}{n}}.$$ The quantity inside the square root above is the average squared deviation which is also called the variance denoted as $$\\text{var}(\\mathbf{x}) = \\frac{\\left[x_1-\\text{avg}(\\mathbf{x})\\right]^2+\\cdots+\\left[x_n-\\text{avg}(\\mathbf{x})\\right]^2}{n}.$$\n",
    "\n",
    "This means $\\text{std}(\\mathbf{x}) = \\sqrt{\\text{var}(\\mathbf{x})}.$\n",
    "\n",
    "A large standard deviation indicates that the components of the vector typically deviate a lot from their average value or mean.\n",
    "\n",
    "The following component plot of a vector of heart rate values has the 1-standard deviation-above and below the mean represented as red-dotted lines:\n",
    "\n",
    "![standard deviation](https://1drv.ms/i/c/37720f927b6ddc34/IQQB_uF-TUO8SpoodLWz7sQPAc4POmYfY3hPjlX3vpYfKlY?width=540)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart rate vector:\n",
      " tensor([72., 85., 68., 90., 84., 78.], dtype=torch.float64)\n",
      "Average heart rate: 79.5\n",
      "Deviations in heart rate vector:\n",
      " tensor([ -7.5000,   5.5000, -11.5000,  10.5000,   4.5000,  -1.5000],\n",
      "       dtype=torch.float64)\n",
      "Squared-deviations in heart rate vector:\n",
      " tensor([ 56.2500,  30.2500, 132.2500, 110.2500,  20.2500,   2.2500],\n",
      "       dtype=torch.float64)\n",
      "Average squared deviation or variance in the heart rate: 58.583333333333336\n",
      "Standard deviation of the heart rate: 7.65397500213669\n",
      "Standardized heart rate vector:\n",
      "tensor([-0.9799,  0.7186, -1.5025,  1.3718,  0.5879, -0.1960],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Heart rate vector\n",
    "a = X[:, 0]\n",
    "print(f'Heart rate vector:\\n {a}')\n",
    "\n",
    "# BP vector\n",
    "b = X[:, 1]\n",
    "#print(f'Blood pressure vector:\\n {b}')\n",
    "\n",
    "# Average heart rate\n",
    "print(f'Average heart rate: {torch.mean(a)}')\n",
    "\n",
    "# Average BP\n",
    "#print(torch.mean(b))\n",
    "\n",
    "# Mean-centered heart rate vector or the de-meaned heart rate vector or the\n",
    "# deviations in heart rate vectors\n",
    "a_mc = a - torch.mean(a)\n",
    "print(f'Deviations in heart rate vector:\\n {a_mc}')\n",
    "\n",
    "# The average of the components of the mean-centered heart rate vector is zero\n",
    "#print(torch.mean(a_mc))\n",
    "\n",
    "# The squared deviations vector\n",
    "print(f'Squared-deviations in heart rate vector:\\n {a_mc**2}')\n",
    "\n",
    "# The average of the squared deviations vector a.k.a. the variance in\n",
    "# the heart rate\n",
    "v = torch.mean(a_mc**2)\n",
    "print(f'Average squared deviation or variance in the heart rate: {v}')\n",
    "\n",
    "# Square-root of the average of the squared deviations vector\n",
    "# which is the same as the square root of the variance a.k.a. the\n",
    "# standard deviation in the heart rate\n",
    "s = torch.sqrt(v)\n",
    "print(f'Standard deviation of the heart rate: {s}')\n",
    "\n",
    "# Standardized heart rate vector a.k.a. the z-scores of the heart rate is\n",
    "# obtained by subtracting the mean heart rate and dividing by the\n",
    "# deviation of the heart rates\n",
    "z = a_mc / s #same as doing (a - torch.mean(a))/torch.std(a)\n",
    "print(f'Standardized heart rate vector:\\n{z}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Suppose heart rate is measured in beats per hour instead of beats per minute. How do the z-scores look like now?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart rate vector:\n",
      " tensor([4320., 5100., 4080., 5400., 5040., 4680.], dtype=torch.float64)\n",
      "Average heart rate: 4770.0\n",
      "Deviations in heart rate vector:\n",
      " tensor([-450.,  330., -690.,  630.,  270.,  -90.], dtype=torch.float64)\n",
      "Squared-deviations in heart rate vector:\n",
      " tensor([202500., 108900., 476100., 396900.,  72900.,   8100.],\n",
      "       dtype=torch.float64)\n",
      "Average squared deviation or variance in the heart rate: 210900.0\n",
      "Standard deviation of the heart rate: 459.23850012820134\n",
      "Standardized heart rate vector:\n",
      "tensor([-0.9799,  0.7186, -1.5025,  1.3718,  0.5879, -0.1960],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Suppose heart rate is measured in beats per hour instead of beats per minute\n",
    "a = X[:, 0]*60\n",
    "print(f'Heart rate vector:\\n {a}')\n",
    "\n",
    "\n",
    "# Average heart rate\n",
    "print(f'Average heart rate: {torch.mean(a)}')\n",
    "\n",
    "\n",
    "# Mean-centered heart rate vector or the de-meaned heart rate vector or the\n",
    "# deviations in heart rate vectors\n",
    "a_mc = a - torch.mean(a)\n",
    "print(f'Deviations in heart rate vector:\\n {a_mc}')\n",
    "\n",
    "# The average of the components of the mean-centered heart rate vector is zero\n",
    "#print(torch.mean(a_mc))\n",
    "\n",
    "# The squared deviations vector\n",
    "print(f'Squared-deviations in heart rate vector:\\n {a_mc**2}')\n",
    "\n",
    "# The average of the squared deviations vector a.k.a. the variance in\n",
    "# the heart rate\n",
    "v = torch.mean(a_mc**2)\n",
    "print(f'Average squared deviation or variance in the heart rate: {v}')\n",
    "\n",
    "# Square-root of the average of the squared deviations vector\n",
    "# which is the same as the square root of the variance a.k.a. the\n",
    "# standard deviation in the heart rate\n",
    "s = torch.sqrt(v)\n",
    "print(f'Standard deviation of the heart rate: {s}')\n",
    "\n",
    "# Standardized heart rate vector a.k.a. the z-scores of the heart rate\n",
    "z = a_mc / s\n",
    "print(f'Standardized heart rate vector:\\n{z}')\n",
    "# The z-scores are the same as before when the heart rate was in beats per minute"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
