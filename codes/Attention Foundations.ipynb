{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7g7bxFCHxGP"
   },
   "source": [
    "$$\\Large\\boxed{\\text{AME 5202 Deep Learning, Even Semester 2026}}$$\n",
    "\n",
    "$$\\large\\text{Theme}: \\underline{\\text{computational foundations of the self-attention mechanism}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDK9fC6uiBGE"
   },
   "source": [
    "---\n",
    "\n",
    "Load essential libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "20W0d4ruQjE4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfYXkqmLiVLM"
   },
   "source": [
    "---\n",
    "\n",
    "Mount Google Drive folder if running Google Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYzBBBxqiaGa"
   },
   "outputs": [],
   "source": [
    "## Mount Google drive folder if running in Colab\n",
    "if('google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/EvenSem2026MAHE'\n",
    "    DATA_DIR = DIR+'/Data/'\n",
    "else:\n",
    "    DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avVZ6D1ZgEUT"
   },
   "source": [
    "---\n",
    "\n",
    "Some LLM Magic\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "sentence = \"I swam across the river to the other [MASK].\"\n",
    "#sentence = \"I ran across the street to get cash from the [MASK].\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased') # about 440MB large\n",
    "\n",
    "inputs = tokenizer(sentence, return_tensors = \"pt\")\n",
    "logits = model(**inputs).logits\n",
    "predicted_id = logits[0, inputs.input_ids[0] == tokenizer.mask_token_id].argmax(dim=-1)\n",
    "\n",
    "print(tokenizer.decode(predicted_id))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here we create a simple sentence in English and tokenize it\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'i swam quickly across the river to get to the other bank'\n",
    "nltk.download('punkt_tab')\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Download the word embedding model trained on Wikipedia articles, generate the word embeddings for the tokens in the sentence above, and store them in a matrix $\\mathbf{X}$ such that each row of the matrix corresponds to a token.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wikipedia-trained GLoVe word vectors (50-dimensional) from the pickle file\n",
    "with open(DATA_DIR + 'glove_wiki_gigaword_50.pkl', 'rb') as f:\n",
    "    loaded_word_vectors = pickle.load(f)\n",
    "\n",
    "X = np.empty((len(tokens), 50))\n",
    "X = torch.stack([torch.tensor(loaded_word_vectors.get(token, None),\n",
    "                               dtype = torch.float64)\n",
    "                                 for token in tokens])\n",
    "print(X.shape)\n",
    "print(X[1]) # embedding vector for the word \"swam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Calculate the similarities between the words (1) swam and bank (2) river and bank. What do you observe?\n",
    "\n",
    "$$\\texttt{tokens = ['i',\n",
    " 'swam',\n",
    " 'quickly',\n",
    " 'across',\n",
    " 'the',\n",
    " 'river',\n",
    " 'to',\n",
    " 'get',\n",
    " 'to',\n",
    " 'the',\n",
    " 'other',\n",
    " 'bank']}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token-1 is 'swam', token-5 is 'river', and token-11 is 'bank'\n",
    "print(torch.dot(X[1], X[-1])) # similarity between 'swam' and 'bank'\n",
    "print(torch.dot(X[5], X[-1])) # similarity between 'river' and 'bank'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The similarity between each pair of words represented in the word embeddings matrix $\\mathbf{X}$ is the matrix-matrix product $\\mathbf{X}\\mathbf{X}^\\mathrm{T}.$ This is because:\n",
    "\n",
    "- the $(i,j)\\text{th}$ element of the matrix $\\mathbf{X}\\mathbf{X}^\\mathrm{T}$ is the dot-product between the $i\\text{th}$ row of $\\mathbf{X}$ and the $j\\text{th}$ column of $\\mathbf{X}^\\mathrm{T}.$\n",
    "- But the $j\\text{th}$ column of $\\mathbf{X}^\\mathrm{T}$ is also the $j\\text{th}$ row of $\\mathbf{X}.$\n",
    "- Therefore, $\\left[\\mathbf{X}\\mathbf{X}^\\mathrm{T}\\right]_{i,j} = \\mathbf{x}^{(i)}\\cdot\\mathbf{x}^{(j)},$ which is the similarity between words $i$ and $j$ calculated using their embeddings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.matmul(X, X.T)\n",
    "print(S[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The similarity matrix can be scaled (values from 0 through 1) by applying softmax row-by-row to the matrix $\\mathbf{X}\\mathbf{X}^\\mathrm{T}.$ The resulting values are called the scaled similarity scores or more commonly as the attention coefficients.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled similarities\n",
    "S_scaled = ?\n",
    "\n",
    "# Transformed or new embeddings for the word swam\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The transformed embeddings matrix can be calculated as ${\\mathbf{Y}} = \\text{softmax}\\left(\\mathbf{X}\\mathbf{X}^\\mathrm{T}\\right)\\mathbf{X}.$\n",
    "\n",
    "Note that the $i\\text{th}$ row of $\\mathbf{Y}$ is a linear combination of the rows of $\\mathbf{X}$ (the original embeddings for the words) using the attention coefficients corresponding to the $i\\text{th}$ word--which are in the $i\\text{th}$ row of the matrix $\\text{softmax}\\left(\\mathbf{X}\\mathbf{X}^\\mathrm{T}\\right)$-- as multipliers.\n",
    "\n",
    "Thus, the $i\\text{th}$ row of $\\mathbf{Y}$ represents the new or transformed embeddings for the $i\\text{th}$ word.\n",
    "\n",
    "Intuitively, the new embeddings matrix ${\\mathbf{Y}}$ (a *transformed* version of $\\mathbf{X}$) captures the contextual similarities between the words in the sentence well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The attention mechanism with learning:\n",
    "\n",
    "![](https://1drv.ms/i/c/37720f927b6ddc34/IQSRMkyKcxZaRqR5kaD6nSJTAcUM0iMFoJ_z8DvjdkB_olw?width=550)\n",
    "\n",
    "- When processing a sentence, each word ${\\color{red}{\\text{queries}}}$ all other words using the learned ${\\color{red}{\\text{query weights}}}$ ${\\color{red}{\\mathbf{W^{(q)}}}}.$\n",
    "- Each word provides ${\\color{blue}{\\text{keys}}}$ to see if it matches a query, based on the learned ${\\color{blue}{\\text{key weights}}}$ ${\\color{blue}{\\mathbf{W^{(k)}}}}.$\n",
    "- If a match is found, the ${\\color{cyan}{\\text{value weights}}}$ ${\\color{cyan}{\\mathbf{W^{(v)}}}}$determine how much of the word's information is passed to the final attention output.\n",
    "\n",
    "The result of this process is to (re)produce context-aware embeddings for each word. As an example, consider the following sentence ``*the bank raised interest rates*'' with the initial embeddings for\n",
    "$$\\begin{align*}\n",
    "\\textit{the} &= [0.1, 0.2, 0.1],\\\\\n",
    "\\textit{bank} &= [0.4, 0.5, 0.3],\\\\\n",
    "\\textit{raised} &= [0.1, 0.3, 0.4],\\\\\n",
    "\\textit{interest} &= [0.7, 0.1, 0.3],\\\\\n",
    "\\textit{rates} &= [0.2, 0.4, 0.5].\n",
    "\\end{align*}$$\n",
    "\n",
    "1. Calculate the pairwise scaled similarities between all the words assuming no learning using the query, key, and value weights.\n",
    "\\question What is the scaled similarity between the words *bank* and *interest*? Is the similarity strong or weak given the financial context?\n",
    "2. Now consider the following query, key and value weights:\n",
    "$$\\begin{align*}\n",
    "{\\color{red}{\\mathbf{W^{(q)}}}} &= \\begin{bmatrix}\n",
    "1.8 & 4.5 & 3.9 \\\\\n",
    "4.3 & 0.2 & 2.9 \\\\\n",
    "5.0 & 0.8 & 4.8\n",
    "\\end{bmatrix},\n",
    "\\ {\\color{blue}{\\mathbf{W^{(k)}}}} =\\begin{bmatrix}\n",
    "2.7 & 3.7 & 3.2\\\\\n",
    "0.2 & 1.9 & 3.6\\\\\n",
    "2.6 & 4.9 &1.9\n",
    "\\end{bmatrix},\n",
    "\\ {\\color{cyan}{\\mathbf{W^{(v)}}}} =\\begin{bmatrix}\n",
    "0.5 & 2.6 & 2.4\\\\\n",
    "3.6 & 3.1 & 3.2\\\\\n",
    "0.3 & 0.6 & 0.9\n",
    "\\end{bmatrix}{\\color{black}.}\n",
    "\\end{align*}$$\n",
    "Write down the queries, keys, and values associated with the words *bank* and *interest*.\n",
    "3. Calculate the pairwise scaled similarities between all the words using the query, key, and value weights above.\n",
    "4. What is the scaled similarity between the words *bank* and *interest* now? Is the similarity strong or weak given the financial context?\n",
    "5. In calculating the transformed embeddings for the word *bank*, which two words are weighted the most, and what are those weights?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original embeddings of the words\n",
    "the = torch.tensor([0.1, 0.2, 0.1])\n",
    "bank = torch.tensor([0.4, 0.5, 0.3])\n",
    "raised = torch.tensor([0.1, 0.3, 0.4])\n",
    "interest = torch.tensor([0.7, 0.1, 0.3])\n",
    "rates = torch.tensor([0.2, 0.4, 0.5])\n",
    "\n",
    "# Sample\n",
    "X = torch.vstack([the, bank, raised, interest, rates])\n",
    "print(f'Original embeddings: \\n{X}')\n",
    "\n",
    "# Scaled similarities with no learning\n",
    "S = torch.nn.functional.softmax(torch.matmul(X, X.T), dim = -1)\n",
    "print(f'Scaled similarities with no learning:\\n {S}')\n",
    "\n",
    "# Query, key, and value neurons' weights matrices\n",
    "W_q = torch.tensor([[1.8, 4.5, 3.9],\n",
    "                    [4.3, 0.2, 2.9],\n",
    "                    [5.0, 0.8, 4.8]], dtype = torch.float32)\n",
    "\n",
    "W_k = torch.tensor([[2.7, 3.7, 3.2],\n",
    "                    [0.2, 1.9, 3.6],\n",
    "                    [2.6, 4.9, 1.9]], dtype = torch.float32)\n",
    "\n",
    "W_v = torch.tensor([[0.5, 2.6, 2.4],\n",
    "                    [3.6, 3.1, 3.2],\n",
    "                    [0.3, 0.6, 0.9]], dtype = torch.float32)\n",
    "\n",
    "# Calculate learned queries, keys, nad values\n",
    "Q = torch.matmul(X, W_q)\n",
    "K = torch.matmul(X, W_k)\n",
    "V = torch.matmul(X, W_v)\n",
    "\n",
    "# Calculate scaled similarities with learning\n",
    "d_k = K.shape[-1]\n",
    "S_learned = torch.nn.functional.softmax(torch.matmul(Q, K.T)/torch.sqrt(torch.tensor(d_k, dtype = torch.float32)), dim = -1)\n",
    "print(f'Scaled similarities with learning:\\n {S_learned}')\n",
    "\n",
    "# Calculate transformed embeddings of the words\n",
    "Y = torch.matmul(S_learned, V)\n",
    "print(f'Transformed embeddings: \\n{Y}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
