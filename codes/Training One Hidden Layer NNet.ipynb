{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7g7bxFCHxGP"
   },
   "source": [
    "$$\\Large\\boxed{\\text{AME 5202 Deep Learning, Even Semester 2026}}$$\n",
    "\n",
    "$$\\large\\text{Theme}: \\underline{\\text{Training a simple 1-hidden layer neural network for classification and regression}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDK9fC6uiBGE"
   },
   "source": [
    "---\n",
    "\n",
    "Load essential libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "20W0d4ruQjE4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfYXkqmLiVLM"
   },
   "source": [
    "---\n",
    "\n",
    "Mount Google Drive folder if running Google Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYzBBBxqiaGa"
   },
   "outputs": [],
   "source": [
    "## Mount Google drive folder if running in Colab\n",
    "if('google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/EvenSem2026MAHE'\n",
    "    DATA_DIR = DIR+'/Data/'\n",
    "else:\n",
    "    DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Patient data matrix with output labels:\n",
    "\n",
    "![patient dataset](https://1drv.ms/i/c/37720f927b6ddc34/UQQ03G17kg9yIIA3SKYBAAAAAJC-Ffd_ghqvEZE?width=800)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient data matrix with 6 samples and 5 features:\n",
      "tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n",
      "        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n",
      "        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n",
      "        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n",
      "        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n",
      "        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]],\n",
      "       dtype=torch.float64)\n",
      "-----\n",
      "True output labels vector:\n",
      " ['non-diabetic' 'diabetic' 'non-diabetic' 'pre-diabetic' 'diabetic'\n",
      " 'pre-diabetic']\n",
      "-----\n",
      "One-hot encoded output labels matrix:\n",
      " tensor([[0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.]], dtype=torch.float64)\n",
      "-----\n",
      "The standardized data matrix:\n",
      "tensor([[-0.9799, -0.7019, -0.7238, -0.9871,  0.8920],\n",
      "        [ 0.7186,  0.3509, -1.2449, -0.6050, -1.2374],\n",
      "        [-1.5025, -1.7547,  1.3607,  0.3503,  1.0647],\n",
      "        [ 1.3718,  1.4037,  0.4922,  0.6687,  0.1439],\n",
      "        [ 0.5879,  0.5615,  1.0133,  1.6876,  0.6043],\n",
      "        [-0.1960,  0.1404, -0.8975, -1.1144, -1.4676]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "## Create the patient data matrix as a constant tensor\n",
    "X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n",
    "                  [85, 130, 37.0, 110, 14],\n",
    "                  [68, 110, 38.5, 125, 34],\n",
    "                  [90, 140, 38.0, 130, 26],\n",
    "                  [84, 132, 38.3, 146, 30],\n",
    "                  [78, 128, 37.2, 102, 12]],\n",
    "                  dtype = torch.float64)\n",
    "print(f'Patient data matrix with 6 samples and 5 features:\\n{X}')\n",
    "print('-----')\n",
    "\n",
    "# Create a 1D-numpy array of output labels (equivalent to a rank-1 tensor in\n",
    "# PyTorch which itself is equivalent to a vector in pen & paper)\n",
    "y = np.array(['non-diabetic',\n",
    "              'diabetic',\n",
    "              'non-diabetic',\n",
    "              'pre-diabetic',\n",
    "              'diabetic',\n",
    "              'pre-diabetic'])\n",
    "print(f'True output labels vector:\\n {y}')\n",
    "print('-----')\n",
    "\n",
    "# Creating a one-hot encoder object\n",
    "ohe = OneHotEncoder(sparse_output = False)\n",
    "\n",
    "# Create the one-hot encoded true output labels matrix\n",
    "Y = torch.tensor(ohe.fit_transform(y.reshape(-1, 1)), dtype = torch.float64)\n",
    "print(f'One-hot encoded output labels matrix:\\n {Y}')\n",
    "print('-----')\n",
    "\n",
    "# Standardize the data\n",
    "sc = StandardScaler() # create a standard scaler object\n",
    "X_std = torch.tensor(sc.fit_transform(X), dtype = torch.float64)\n",
    "print(f'The standardized data matrix:\\n{X_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The ReLU (rectified linear unit) non-linear activation function: $$\\begin{align*}g(z)&=\\begin{cases}z,&\\text{if }z\\geq0,\\\\0,&\\text{if }z<0.\\end{cases}\\end{align*}$$\n",
    "\n",
    "Lets non-negative raw scores flow through and clips negative raw scores to zero:\n",
    "\n",
    "![ReLU graph](https://1drv.ms/i/c/37720f927b6ddc34/IQQ2LpqSl9JMSKCzneksEc8CAXrRgpiM7O00yEnO9YUtbZA?width=562&height=200)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined function for pointwise ReLU activation\n",
    "def ReLU(z):\n",
    "  return(z * (z >= 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "A one-hidden layer neural network, or equivalently a 2-layer neural network with pointwise ReLU activation for the hidden layer:\n",
    "\n",
    "\n",
    "![2-layer neural network](https://1drv.ms/i/c/37720f927b6ddc34/IQTcIoux_NYvTKiIUEK_P3ffAY0ViDllOLNX8bhgeWSqE5g?width=660)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw scores matrix for layer-1 with shape [6, 4]:\n",
      "tensor([[ 0.0479, -0.0249, -0.0030,  0.0062],\n",
      "        [-0.0119, -0.0089, -0.0056, -0.0272],\n",
      "        [ 0.0113,  0.0294,  0.0104,  0.0500],\n",
      "        [-0.0208, -0.0044, -0.0024, -0.0114],\n",
      "        [-0.0319,  0.0281, -0.0018,  0.0015],\n",
      "        [ 0.0055, -0.0192,  0.0025, -0.0190]], dtype=torch.float64,\n",
      "       grad_fn=<MmBackward0>)\n",
      "-----\n",
      "The ReLU-activated scores matrix for layer-1 with shape [6, 4]:\n",
      "tensor([[0.0479, -0.0000, -0.0000, 0.0062],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [0.0113, 0.0294, 0.0104, 0.0500],\n",
      "        [-0.0000, -0.0000, -0.0000, -0.0000],\n",
      "        [-0.0000, 0.0281, -0.0000, 0.0015],\n",
      "        [0.0055, -0.0000, 0.0025, -0.0000]], dtype=torch.float64,\n",
      "       grad_fn=<MulBackward0>)\n",
      "-----\n",
      "The raw scores matrix for layer-2 with shape [6, 3]:\n",
      "tensor([[-4.8773e-04, -2.4236e-04,  5.0855e-04],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-6.9533e-04,  9.9394e-04,  1.5483e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 2.4834e-05,  1.5720e-04,  2.9198e-04],\n",
      "        [-3.8151e-05, -5.8284e-05,  5.7716e-05]], dtype=torch.float64,\n",
      "       grad_fn=<MmBackward0>)\n",
      "-----\n",
      "The softmax-activated scores matrix for layer-2 with shape [6, 3]:\n",
      "tensor([[0.3332, 0.3333, 0.3335],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3329, 0.3335, 0.3336],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3334],\n",
      "        [0.3333, 0.3333, 0.3334]], dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate raw scores for layer-1 by randomly initializing the weights matrix for layer-1\n",
    "W1 = torch.nn.Parameter(torch.randn(5, 4, dtype = torch.float64)*0.01)\n",
    "Z1 = X_std @ W1 # torch.matmul(X_std, W1)\n",
    "#W1 = torch.linear(5, 4)\n",
    "#Z1 = W1(X_std) \n",
    "print(f'The raw scores matrix for layer-1 with shape {list(Z1.shape)}:\\n{Z1}')\n",
    "print('-----')\n",
    "\n",
    "# ReLU-activate the raw scores in layer-1\n",
    "A1 = ReLU(Z1)\n",
    "print(f'The ReLU-activated scores matrix for layer-1 with shape {list(A1.shape)}:\\n{A1}')\n",
    "print('-----')\n",
    "\n",
    "# Calculate raw scores for layer-2 by randomly initializing the weights matrix for layer-2\n",
    "W2 = torch.nn.Parameter(torch.randn(4, 3, dtype = torch.float64)*0.01)\n",
    "Z2 = A1 @ W2 \n",
    "print(f'The raw scores matrix for layer-2 with shape {list(Z2.shape)}:\\n{Z2}')\n",
    "print('-----')\n",
    "\n",
    "# Softmax-activate the raw scores in layer-2\n",
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "A2 = softmax(Z2)\n",
    "print(f'The softmax-activated scores matrix for layer-2 with shape {list(A2.shape)}:\\n{A2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avVZ6D1ZgEUT"
   },
   "source": [
    "---\n",
    "\n",
    "Using PyTorch, calculate the optimal weights $\\mathbf{W}^{[1]}$ and $\\mathbf{W}^{[2]}$ for the 2-layer neural network\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 1.0986294792198172\n",
      "Epoch 1, loss = 1.0979071284380881\n",
      "Epoch 2, loss = 1.0967686109876016\n",
      "Epoch 3, loss = 1.095101427686352\n",
      "Epoch 4, loss = 1.0927046450186169\n",
      "Epoch 5, loss = 1.0895495359113303\n",
      "Epoch 6, loss = 1.0855845164913422\n",
      "Epoch 7, loss = 1.080740554196041\n",
      "Epoch 8, loss = 1.0749715647129587\n",
      "Epoch 9, loss = 1.0682483565205196\n",
      "Epoch 10, loss = 1.0605504389978415\n",
      "Epoch 11, loss = 1.0518648745367045\n",
      "Epoch 12, loss = 1.0424468642621452\n",
      "Epoch 13, loss = 1.0321372011988335\n",
      "Epoch 14, loss = 1.020926948327578\n",
      "Epoch 15, loss = 1.0088409987140445\n",
      "Epoch 16, loss = 0.9959162651419136\n",
      "Epoch 17, loss = 0.9821992382092221\n",
      "Epoch 18, loss = 0.9677451168173814\n",
      "Epoch 19, loss = 0.9526172976507263\n",
      "Epoch 20, loss = 0.9368868694999222\n",
      "Epoch 21, loss = 0.9206319319276058\n",
      "Epoch 22, loss = 0.903936622342744\n",
      "Epoch 23, loss = 0.887293684938658\n",
      "Epoch 24, loss = 0.8700334802916051\n",
      "Epoch 25, loss = 0.8523988112174083\n",
      "Epoch 26, loss = 0.8349545757717166\n",
      "Epoch 27, loss = 0.817617694918657\n",
      "Epoch 28, loss = 0.8003779438774011\n",
      "Epoch 29, loss = 0.783314936100782\n",
      "Epoch 30, loss = 0.7664985662119307\n",
      "Epoch 31, loss = 0.749987820515514\n",
      "Epoch 32, loss = 0.7338299946428254\n",
      "Epoch 33, loss = 0.7180603707450041\n",
      "Epoch 34, loss = 0.7031618254583\n",
      "Epoch 35, loss = 0.6884925579047029\n",
      "Epoch 36, loss = 0.6739528969897997\n",
      "Epoch 37, loss = 0.6602869063994911\n",
      "Epoch 38, loss = 0.6471413493824593\n",
      "Epoch 39, loss = 0.6343732254949241\n",
      "Epoch 40, loss = 0.6219671742091845\n",
      "Epoch 41, loss = 0.6099065699220753\n",
      "Epoch 42, loss = 0.5981746413828027\n",
      "Epoch 43, loss = 0.5868970408840396\n",
      "Epoch 44, loss = 0.5761706086490215\n",
      "Epoch 45, loss = 0.5656223983090995\n",
      "Epoch 46, loss = 0.5555776830972713\n",
      "Epoch 47, loss = 0.5457740066952702\n",
      "Epoch 48, loss = 0.5366531336726258\n",
      "Epoch 49, loss = 0.5275235025281374\n",
      "Epoch 50, loss = 0.5187015896059884\n",
      "Epoch 51, loss = 0.5104045619667942\n",
      "Epoch 52, loss = 0.5023025527902963\n",
      "Epoch 53, loss = 0.4944000668307605\n",
      "Epoch 54, loss = 0.48746101254020385\n",
      "Epoch 55, loss = 0.48041258062467956\n",
      "Epoch 56, loss = 0.4732758562803987\n",
      "Epoch 57, loss = 0.4668897720055119\n",
      "Epoch 58, loss = 0.4609264546048033\n",
      "Epoch 59, loss = 0.4550947577071946\n",
      "Epoch 60, loss = 0.44940068086858914\n",
      "Epoch 61, loss = 0.44385047013449164\n",
      "Epoch 62, loss = 0.4385281329413189\n",
      "Epoch 63, loss = 0.4337640192913516\n",
      "Epoch 64, loss = 0.4291737289594943\n",
      "Epoch 65, loss = 0.42484850888762854\n",
      "Epoch 66, loss = 0.42060683629795054\n",
      "Epoch 67, loss = 0.4166486230999939\n",
      "Epoch 68, loss = 0.41274813977436287\n",
      "Epoch 69, loss = 0.4091646910620497\n",
      "Epoch 70, loss = 0.40583098956854274\n",
      "Epoch 71, loss = 0.4025862933715614\n",
      "Epoch 72, loss = 0.3993662295982821\n",
      "Epoch 73, loss = 0.39617669120762256\n",
      "Epoch 74, loss = 0.39346919551547793\n",
      "Epoch 75, loss = 0.39047476261208897\n",
      "Epoch 76, loss = 0.38768718768541105\n",
      "Epoch 77, loss = 0.38512156841287665\n",
      "Epoch 78, loss = 0.38252048802643523\n",
      "Epoch 79, loss = 0.379888046268912\n",
      "Epoch 80, loss = 0.3771203031344536\n",
      "Epoch 81, loss = 0.3737291500730868\n",
      "Epoch 82, loss = 0.37055451993705907\n",
      "Epoch 83, loss = 0.36717195352619497\n",
      "Epoch 84, loss = 0.3640276624053838\n",
      "Epoch 85, loss = 0.3609655222225907\n",
      "Epoch 86, loss = 0.35789215025535054\n",
      "Epoch 87, loss = 0.35482339447138794\n",
      "Epoch 88, loss = 0.35177223695176857\n",
      "Epoch 89, loss = 0.34875028241823397\n",
      "Epoch 90, loss = 0.34576880294343487\n",
      "Epoch 91, loss = 0.3428393297694922\n",
      "Epoch 92, loss = 0.33997378339878126\n",
      "Epoch 93, loss = 0.3376781929590349\n",
      "Epoch 94, loss = 0.3350224610887656\n",
      "Epoch 95, loss = 0.33222184865319676\n",
      "Epoch 96, loss = 0.3299118361386911\n",
      "Epoch 97, loss = 0.32768098339533286\n",
      "Epoch 98, loss = 0.32552820102537566\n",
      "Epoch 99, loss = 0.32344501080824434\n",
      "Epoch 100, loss = 0.32141558505032203\n",
      "Epoch 101, loss = 0.31941775483046114\n",
      "Epoch 102, loss = 0.31742494783928304\n",
      "Epoch 103, loss = 0.31540876947008517\n",
      "Epoch 104, loss = 0.31363677876810947\n",
      "Epoch 105, loss = 0.31158995624161756\n",
      "Epoch 106, loss = 0.309278560038794\n",
      "Epoch 107, loss = 0.3070277129660194\n",
      "Epoch 108, loss = 0.30473487155348716\n",
      "Epoch 109, loss = 0.3022963101944037\n",
      "Epoch 110, loss = 0.2997106377482685\n",
      "Epoch 111, loss = 0.2969780604433349\n",
      "Epoch 112, loss = 0.294099228507867\n",
      "Epoch 113, loss = 0.29107443061157173\n",
      "Epoch 114, loss = 0.2879031368998764\n",
      "Epoch 115, loss = 0.2845838103561313\n",
      "Epoch 116, loss = 0.2811138856880239\n",
      "Epoch 117, loss = 0.27767561601673674\n",
      "Epoch 118, loss = 0.2740195456968251\n",
      "Epoch 119, loss = 0.2700811233994905\n",
      "Epoch 120, loss = 0.26593073026445296\n",
      "Epoch 121, loss = 0.26175917673342186\n",
      "Epoch 122, loss = 0.2573934601032926\n",
      "Epoch 123, loss = 0.2528322410227303\n",
      "Epoch 124, loss = 0.24807577508754533\n",
      "Epoch 125, loss = 0.24312627760510389\n",
      "Epoch 126, loss = 0.23798820156152733\n",
      "Epoch 127, loss = 0.2326709600211887\n",
      "Epoch 128, loss = 0.22722302127472796\n",
      "Epoch 129, loss = 0.22161714733589843\n",
      "Epoch 130, loss = 0.2158933243436807\n",
      "Epoch 131, loss = 0.21003491609052405\n",
      "Epoch 132, loss = 0.20405512875582907\n",
      "Epoch 133, loss = 0.19797486114010207\n",
      "Epoch 134, loss = 0.19181636326155813\n",
      "Epoch 135, loss = 0.18560282428438282\n",
      "Epoch 136, loss = 0.17948284359242186\n",
      "Epoch 137, loss = 0.17323546688132616\n",
      "Epoch 138, loss = 0.16698957028630354\n",
      "Epoch 139, loss = 0.1608585645942931\n",
      "Epoch 140, loss = 0.15477653384181486\n",
      "Epoch 141, loss = 0.14876360134928784\n",
      "Epoch 142, loss = 0.1428380539437283\n",
      "Epoch 143, loss = 0.13701622164251312\n",
      "Epoch 144, loss = 0.13131247141367794\n",
      "Epoch 145, loss = 0.12573929926939134\n",
      "Epoch 146, loss = 0.12036447459129741\n",
      "Epoch 147, loss = 0.11509264190317224\n",
      "Epoch 148, loss = 0.11000148297716288\n",
      "Epoch 149, loss = 0.10509910396035983\n",
      "Epoch 150, loss = 0.10036015392357922\n",
      "Epoch 151, loss = 0.09579006171828348\n",
      "Epoch 152, loss = 0.09139317784002017\n",
      "Epoch 153, loss = 0.08717271453604185\n",
      "Epoch 154, loss = 0.08313068011826667\n",
      "Epoch 155, loss = 0.07926783083398835\n",
      "Epoch 156, loss = 0.07558365834561363\n",
      "Epoch 157, loss = 0.07215844904082096\n",
      "Epoch 158, loss = 0.06886017384800783\n",
      "Epoch 159, loss = 0.06569228462730674\n",
      "Epoch 160, loss = 0.06268603276148847\n",
      "Epoch 161, loss = 0.059890203552003606\n",
      "Epoch 162, loss = 0.05724232291199493\n",
      "Epoch 163, loss = 0.054736439535444326\n",
      "Epoch 164, loss = 0.05236635117669225\n",
      "Epoch 165, loss = 0.05012573275239295\n",
      "Epoch 166, loss = 0.04800824142555148\n",
      "Epoch 167, loss = 0.046010180619050815\n",
      "Epoch 168, loss = 0.044164140672509865\n",
      "Epoch 169, loss = 0.04241021438036218\n",
      "Epoch 170, loss = 0.04074595186247765\n",
      "Epoch 171, loss = 0.039168871468626625\n",
      "Epoch 172, loss = 0.037676089653005736\n",
      "Epoch 173, loss = 0.03628066393273859\n",
      "Epoch 174, loss = 0.034973408631119314\n",
      "Epoch 175, loss = 0.03373546144434415\n",
      "Epoch 176, loss = 0.032562722752554604\n",
      "Epoch 177, loss = 0.03145131408551574\n",
      "Epoch 178, loss = 0.03039757610213555\n",
      "Epoch 179, loss = 0.029398063095188817\n",
      "Epoch 180, loss = 0.028466325028469446\n",
      "Epoch 181, loss = 0.027583519850587736\n",
      "Epoch 182, loss = 0.026743999681019526\n",
      "Epoch 183, loss = 0.025945585973672077\n",
      "Epoch 184, loss = 0.02518616875821859\n",
      "Epoch 185, loss = 0.02446370712809337\n",
      "Epoch 186, loss = 0.023776230487478248\n",
      "Epoch 187, loss = 0.02312184039630343\n",
      "Epoch 188, loss = 0.022505824418798875\n",
      "Epoch 189, loss = 0.021920409729979964\n",
      "Epoch 190, loss = 0.02135951484848342\n",
      "Epoch 191, loss = 0.020821827848688187\n",
      "Epoch 192, loss = 0.020306121111945755\n",
      "Epoch 193, loss = 0.019815975935348666\n",
      "Epoch 194, loss = 0.01935063492605343\n",
      "Epoch 195, loss = 0.018904590797418904\n",
      "Epoch 196, loss = 0.018476841496512842\n",
      "Epoch 197, loss = 0.01806644050247897\n",
      "Epoch 198, loss = 0.01767249288284194\n",
      "Epoch 199, loss = 0.017294151765954616\n",
      "Epoch 200, loss = 0.016930615243472048\n",
      "Epoch 201, loss = 0.016581123677850855\n",
      "Epoch 202, loss = 0.01624495736504371\n",
      "Epoch 203, loss = 0.01592289108205967\n",
      "Epoch 204, loss = 0.015613081831350692\n",
      "Epoch 205, loss = 0.015312431083607303\n",
      "Epoch 206, loss = 0.01502372426762645\n",
      "Epoch 207, loss = 0.014745709852397837\n",
      "Epoch 208, loss = 0.01447710055559817\n",
      "Epoch 209, loss = 0.014217455863354172\n",
      "Epoch 210, loss = 0.013966359295660098\n",
      "Epoch 211, loss = 0.013723416778964895\n",
      "Epoch 212, loss = 0.013488255179622372\n",
      "Epoch 213, loss = 0.013260520985771225\n",
      "Epoch 214, loss = 0.013039879122782913\n",
      "Epoch 215, loss = 0.012826011886136574\n",
      "Epoch 216, loss = 0.012618617975359498\n",
      "Epoch 217, loss = 0.012417411613327533\n",
      "Epoch 218, loss = 0.01222212173654078\n",
      "Epoch 219, loss = 0.012032491243754375\n",
      "Epoch 220, loss = 0.01184837219332858\n",
      "Epoch 221, loss = 0.01166935612771042\n",
      "Epoch 222, loss = 0.011495357191329401\n",
      "Epoch 223, loss = 0.011326078195123162\n",
      "Epoch 224, loss = 0.011161328017787965\n",
      "Epoch 225, loss = 0.0110009251497344\n",
      "Epoch 226, loss = 0.010844697170830806\n",
      "Epoch 227, loss = 0.010692480258052604\n",
      "Epoch 228, loss = 0.010544118720656344\n",
      "Epoch 229, loss = 0.010399464560896299\n",
      "Epoch 230, loss = 0.010258377058599435\n",
      "Epoch 231, loss = 0.01012072237812753\n",
      "Epoch 232, loss = 0.009986373196419951\n",
      "Epoch 233, loss = 0.009855208350929831\n",
      "Epoch 234, loss = 0.009727112506357882\n",
      "Epoch 235, loss = 0.00960197583917155\n",
      "Epoch 236, loss = 0.009479693738965754\n",
      "Epoch 237, loss = 0.009360166525789767\n",
      "Epoch 238, loss = 0.009243299182625741\n",
      "Epoch 239, loss = 0.009129001102265633\n",
      "Epoch 240, loss = 0.009017185847885181\n",
      "Epoch 241, loss = 0.008907770926664905\n",
      "Epoch 242, loss = 0.008800677575847357\n",
      "Epoch 243, loss = 0.008695830560656335\n",
      "Epoch 244, loss = 0.008593157983531687\n",
      "Epoch 245, loss = 0.008492591104155045\n",
      "Epoch 246, loss = 0.008394064169758199\n",
      "Epoch 247, loss = 0.008297514255218448\n",
      "Epoch 248, loss = 0.008202881112456087\n",
      "Epoch 249, loss = 0.008110107028657774\n",
      "Epoch 250, loss = 0.008019136692857953\n",
      "Epoch 251, loss = 0.007929917070419998\n",
      "Epoch 252, loss = 0.007842397284969518\n",
      "Epoch 253, loss = 0.007756528507346129\n",
      "Epoch 254, loss = 0.007672263851151939\n",
      "Epoch 255, loss = 0.0075895582744941415\n",
      "Epoch 256, loss = 0.00750836848753552\n",
      "Epoch 257, loss = 0.0074286528654837715\n",
      "Epoch 258, loss = 0.007350371366676209\n",
      "Epoch 259, loss = 0.007273485455428011\n",
      "Epoch 260, loss = 0.007197958029342067\n",
      "Epoch 261, loss = 0.007123753350788815\n",
      "Epoch 262, loss = 0.00705083698229101\n",
      "Epoch 263, loss = 0.006979175725562329\n",
      "Epoch 264, loss = 0.006908737563967693\n",
      "Epoch 265, loss = 0.006839491608188975\n",
      "Epoch 266, loss = 0.006771408044894126\n",
      "Epoch 267, loss = 0.0067044580882227285\n",
      "Epoch 268, loss = 0.006638613933913659\n",
      "Epoch 269, loss = 0.006573848715910861\n",
      "Epoch 270, loss = 0.006510136465296165\n",
      "Epoch 271, loss = 0.006447452071407609\n",
      "Epoch 272, loss = 0.006385771245009427\n",
      "Epoch 273, loss = 0.006325070483391218\n",
      "Epoch 274, loss = 0.006265327037279981\n",
      "Epoch 275, loss = 0.006206518879455728\n",
      "Epoch 276, loss = 0.006148624674971835\n",
      "Epoch 277, loss = 0.006091623752882354\n",
      "Epoch 278, loss = 0.00603549607938995\n",
      "Epoch 279, loss = 0.005980222232330534\n",
      "Epoch 280, loss = 0.005925783376917082\n",
      "Epoch 281, loss = 0.005872161242670422\n",
      "Epoch 282, loss = 0.005819338101469601\n",
      "Epoch 283, loss = 0.005767296746656228\n",
      "Epoch 284, loss = 0.005716020473136169\n",
      "Epoch 285, loss = 0.005665493058421202\n",
      "Epoch 286, loss = 0.00561569874455866\n",
      "Epoch 287, loss = 0.005566622220900329\n",
      "Epoch 288, loss = 0.005518248607665216\n",
      "Epoch 289, loss = 0.00547056344025203\n",
      "Epoch 290, loss = 0.005423552654262802\n",
      "Epoch 291, loss = 0.005377202571197185\n",
      "Epoch 292, loss = 0.005331499884784025\n",
      "Epoch 293, loss = 0.005286431647915438\n",
      "Epoch 294, loss = 0.005241985260152176\n",
      "Epoch 295, loss = 0.0051981484557706795\n",
      "Epoch 296, loss = 0.005154909292323012\n",
      "Epoch 297, loss = 0.005112256139684741\n",
      "Epoch 298, loss = 0.005070177669564948\n",
      "Epoch 299, loss = 0.00502866284545478\n",
      "Epoch 300, loss = 0.004987700912993047\n",
      "Epoch 301, loss = 0.0049472813907285385\n",
      "Epoch 302, loss = 0.004907394061257986\n",
      "Epoch 303, loss = 0.0048680289627220895\n",
      "Epoch 304, loss = 0.004829176380642732\n",
      "Epoch 305, loss = 0.004790826840083828\n",
      "Epoch 306, loss = 0.004752971098120559\n",
      "Epoch 307, loss = 0.004715600136602895\n",
      "Epoch 308, loss = 0.004678705155198442\n",
      "Epoch 309, loss = 0.0046422775647033745\n",
      "Epoch 310, loss = 0.004606308980606697\n",
      "Epoch 311, loss = 0.004570791216897668\n",
      "Epoch 312, loss = 0.004535716280105819\n",
      "Epoch 313, loss = 0.004501076363560557\n",
      "Epoch 314, loss = 0.004466863841863959\n",
      "Epoch 315, loss = 0.004433071265564466\n",
      "Epoch 316, loss = 0.0043996913560234935\n",
      "Epoch 317, loss = 0.004366717000467374\n",
      "Epoch 318, loss = 0.00433414124721494\n",
      "Epoch 319, loss = 0.004301957301075702\n",
      "Epoch 320, loss = 0.004270158518907267\n",
      "Epoch 321, loss = 0.0042387384053305536\n",
      "Epoch 322, loss = 0.004207690608590965\n",
      "Epoch 323, loss = 0.004177008916563298\n",
      "Epoch 324, loss = 0.00414668725289241\n",
      "Epoch 325, loss = 0.004116719673264853\n",
      "Epoch 326, loss = 0.004087100361805412\n",
      "Epoch 327, loss = 0.004057823627594804\n",
      "Epoch 328, loss = 0.004028883901302232\n",
      "Epoch 329, loss = 0.004000275731928754\n",
      "Epoch 330, loss = 0.003971993783657869\n",
      "Epoch 331, loss = 0.003944032832808115\n",
      "Epoch 332, loss = 0.003916387764883692\n",
      "Epoch 333, loss = 0.0038890535717197097\n",
      "Epoch 334, loss = 0.0038620253487191506\n",
      "Epoch 335, loss = 0.0038352982921763014\n",
      "Epoch 336, loss = 0.0038088676966846415\n",
      "Epoch 337, loss = 0.0037827289526261767\n",
      "Epoch 338, loss = 0.0037568775437381552\n",
      "Epoch 339, loss = 0.003731309044754981\n",
      "Epoch 340, loss = 0.0037060191191226597\n",
      "Epoch 341, loss = 0.0036810035167827667\n",
      "Epoch 342, loss = 0.0036562580720235524\n",
      "Epoch 343, loss = 0.0036317787013955814\n",
      "Epoch 344, loss = 0.0036075614016902843\n",
      "Epoch 345, loss = 0.003583602247978083\n",
      "Epoch 346, loss = 0.0035598973917049972\n",
      "Epoch 347, loss = 0.003536443058844731\n",
      "Epoch 348, loss = 0.003513235548105714\n",
      "Epoch 349, loss = 0.003490271229188992\n",
      "Epoch 350, loss = 0.00346754654109804\n",
      "Epoch 351, loss = 0.003445057990495512\n",
      "Epoch 352, loss = 0.003422802150108785\n",
      "Epoch 353, loss = 0.003400775657179131\n",
      "Epoch 354, loss = 0.003378975211955903\n",
      "Epoch 355, loss = 0.0033573975762331894\n",
      "Epoch 356, loss = 0.0033360395719259656\n",
      "Epoch 357, loss = 0.003314898079688317\n",
      "Epoch 358, loss = 0.003293970037567168\n",
      "Epoch 359, loss = 0.0032732524396956817\n",
      "Epoch 360, loss = 0.0032527423350205806\n",
      "Epoch 361, loss = 0.003232436826065159\n",
      "Epoch 362, loss = 0.0032123330677256965\n",
      "Epoch 363, loss = 0.0031924282661000753\n",
      "Epoch 364, loss = 0.003172719677348316\n",
      "Epoch 365, loss = 0.003153204606583186\n",
      "Epoch 366, loss = 0.0031338804067907243\n",
      "Epoch 367, loss = 0.0031147444777786283\n",
      "Epoch 368, loss = 0.0030957942651536233\n",
      "Epoch 369, loss = 0.003077027259323904\n",
      "Epoch 370, loss = 0.003058440994529493\n",
      "Epoch 371, loss = 0.003040033047896023\n",
      "Epoch 372, loss = 0.003021801038514679\n",
      "Epoch 373, loss = 0.0030037426265440707\n",
      "Epoch 374, loss = 0.00298585551233683\n",
      "Epoch 375, loss = 0.002968137435587085\n",
      "Epoch 376, loss = 0.002950586174500084\n",
      "Epoch 377, loss = 0.0029331995449837175\n",
      "Epoch 378, loss = 0.0029159753998581547\n",
      "Epoch 379, loss = 0.0028989116280878067\n",
      "Epoch 380, loss = 0.002882006154030783\n",
      "Epoch 381, loss = 0.0028652569367074014\n",
      "Epoch 382, loss = 0.002848661969086713\n",
      "Epoch 383, loss = 0.0028322192773914093\n",
      "Epoch 384, loss = 0.0028159269204179366\n",
      "Epoch 385, loss = 0.0027997829888756296\n",
      "Epoch 386, loss = 0.00278378560473919\n",
      "Epoch 387, loss = 0.0027679329206191506\n",
      "Epoch 388, loss = 0.002752223119146632\n",
      "Epoch 389, loss = 0.002736654412371931\n",
      "Epoch 390, loss = 0.00272122504117916\n",
      "Epoch 391, loss = 0.002705933274713638\n",
      "Epoch 392, loss = 0.0026907774098233425\n",
      "Epoch 393, loss = 0.002675755770513103\n",
      "Epoch 394, loss = 0.002660866707412195\n",
      "Epoch 395, loss = 0.002646108597253982\n",
      "Epoch 396, loss = 0.00263147984236776\n",
      "Epoch 397, loss = 0.0026169788701826745\n",
      "Epoch 398, loss = 0.0026026041327424265\n",
      "Epoch 399, loss = 0.0025883541062319336\n",
      "Epoch 400, loss = 0.0025742272905146264\n",
      "Epoch 401, loss = 0.0025602222086798473\n",
      "Epoch 402, loss = 0.0025463374066012367\n",
      "Epoch 403, loss = 0.00253257145250451\n",
      "Epoch 404, loss = 0.0025189229365455124\n",
      "Epoch 405, loss = 0.00250539047039772\n",
      "Epoch 406, loss = 0.0024919726868481225\n",
      "Epoch 407, loss = 0.002478668239403625\n",
      "Epoch 408, loss = 0.0024654758019050167\n",
      "Epoch 409, loss = 0.002452394068149712\n",
      "Epoch 410, loss = 0.0024394217515232588\n",
      "Epoch 411, loss = 0.002426557584638823\n",
      "Epoch 412, loss = 0.0024138003189839046\n",
      "Epoch 413, loss = 0.0024011487245758386\n",
      "Epoch 414, loss = 0.002388601589623789\n",
      "Epoch 415, loss = 0.002376157720198858\n",
      "Epoch 416, loss = 0.0023638159399108328\n",
      "Epoch 417, loss = 0.00235157508959189\n",
      "Epoch 418, loss = 0.0023394340269878443\n",
      "Epoch 419, loss = 0.002327391626454352\n",
      "Epoch 420, loss = 0.002315446778661596\n",
      "Epoch 421, loss = 0.002303598390303673\n",
      "Epoch 422, loss = 0.0022918453838149345\n",
      "Epoch 423, loss = 0.0022801866970918083\n",
      "Epoch 424, loss = 0.0022686212832212283\n",
      "Epoch 425, loss = 0.002257148110213382\n",
      "Epoch 426, loss = 0.0022457661607418696\n",
      "Epoch 427, loss = 0.0022344744318875303\n",
      "Epoch 428, loss = 0.002223271934888638\n",
      "Epoch 429, loss = 0.0022121576948957063\n",
      "Epoch 430, loss = 0.0022011307507318607\n",
      "Epoch 431, loss = 0.0021901901546574442\n",
      "Epoch 432, loss = 0.002179334972140115\n",
      "Epoch 433, loss = 0.0021685642816292095\n",
      "Epoch 434, loss = 0.0021578771743341694\n",
      "Epoch 435, loss = 0.0021472727540093935\n",
      "Epoch 436, loss = 0.002136750136740782\n",
      "Epoch 437, loss = 0.002126308450738428\n",
      "Epoch 438, loss = 0.0021159468361327837\n",
      "Epoch 439, loss = 0.0021056644447752268\n",
      "Epoch 440, loss = 0.002095460440042014\n",
      "Epoch 441, loss = 0.0020853339966426846\n",
      "Epoch 442, loss = 0.0020752843004321737\n",
      "Epoch 443, loss = 0.002065310548225921\n",
      "Epoch 444, loss = 0.0020554119476200246\n",
      "Epoch 445, loss = 0.0020455877168131004\n",
      "Epoch 446, loss = 0.0020358370844336775\n",
      "Epoch 447, loss = 0.002026159289368811\n",
      "Epoch 448, loss = 0.0020165535805977968\n",
      "Epoch 449, loss = 0.002007019217027739\n",
      "Epoch 450, loss = 0.001997555467333621\n",
      "Epoch 451, loss = 0.001988161609799949\n",
      "Epoch 452, loss = 0.001978836932166878\n",
      "Epoch 453, loss = 0.001969580731478321\n",
      "Epoch 454, loss = 0.0019603923139330493\n",
      "Epoch 455, loss = 0.0019512709947390181\n",
      "Epoch 456, loss = 0.0019422160979701102\n",
      "Epoch 457, loss = 0.001933226956425989\n",
      "Epoch 458, loss = 0.0019243029114935906\n",
      "Epoch 459, loss = 0.001915443313012703\n",
      "Epoch 460, loss = 0.0019066475191426707\n",
      "Epoch 461, loss = 0.0018979148962325108\n",
      "Epoch 462, loss = 0.0018892448186929901\n",
      "Epoch 463, loss = 0.0018806366688707392\n",
      "Epoch 464, loss = 0.0018720898369259792\n",
      "Epoch 465, loss = 0.0018636037207110216\n",
      "Epoch 466, loss = 0.0018551777256514863\n",
      "Epoch 467, loss = 0.0018468112646304695\n",
      "Epoch 468, loss = 0.001838503757873637\n",
      "Epoch 469, loss = 0.0018302546328367551\n",
      "Epoch 470, loss = 0.0018220633240962796\n",
      "Epoch 471, loss = 0.001813929273240298\n",
      "Epoch 472, loss = 0.0018058519287623457\n",
      "Epoch 473, loss = 0.0017978307459575243\n",
      "Epoch 474, loss = 0.0017898651868191728\n",
      "Epoch 475, loss = 0.0017819547199388953\n",
      "Epoch 476, loss = 0.0017740988204071988\n",
      "Epoch 477, loss = 0.0017662969697163928\n",
      "Epoch 478, loss = 0.0017585486556653586\n",
      "Epoch 479, loss = 0.0017508533722656679\n",
      "Epoch 480, loss = 0.0017432106196495721\n",
      "Epoch 481, loss = 0.0017356199039791408\n",
      "Epoch 482, loss = 0.0017280807373580443\n",
      "Epoch 483, loss = 0.0017205926377436276\n",
      "Epoch 484, loss = 0.001713155128861544\n",
      "Epoch 485, loss = 0.0017057677401206759\n",
      "Epoch 486, loss = 0.0016984300065311464\n",
      "Epoch 487, loss = 0.0016911414686224793\n",
      "Epoch 488, loss = 0.001683901672363414\n",
      "Epoch 489, loss = 0.0016767101690834656\n",
      "Epoch 490, loss = 0.0016695665153953966\n",
      "Epoch 491, loss = 0.0016624702731195815\n",
      "Epoch 492, loss = 0.0016554210092089875\n",
      "Epoch 493, loss = 0.0016484182956756205\n",
      "Epoch 494, loss = 0.0016414617095187342\n",
      "Epoch 495, loss = 0.001634550832653558\n",
      "Epoch 496, loss = 0.0016276852518416441\n",
      "Epoch 497, loss = 0.0016208645586221003\n",
      "Epoch 498, loss = 0.0016140883492444568\n",
      "Epoch 499, loss = 0.001607356224601924\n",
      "Epoch 500, loss = 0.0016006677901666145\n",
      "Epoch 501, loss = 0.0015940226559249128\n",
      "Epoch 502, loss = 0.0015874204363146448\n",
      "Epoch 503, loss = 0.0015808607501633747\n",
      "Epoch 504, loss = 0.0015743432206265608\n",
      "Epoch 505, loss = 0.0015678674751284781\n",
      "Epoch 506, loss = 0.0015614331453027272\n",
      "Epoch 507, loss = 0.0015550398669338069\n",
      "Epoch 508, loss = 0.0015486872799007876\n",
      "Epoch 509, loss = 0.0015423750281206995\n",
      "Epoch 510, loss = 0.0015361027594931465\n",
      "Epoch 511, loss = 0.0015298701258463613\n",
      "Epoch 512, loss = 0.001523676782883207\n",
      "Epoch 513, loss = 0.0015175223901286322\n",
      "Epoch 514, loss = 0.0015114066108784536\n",
      "Epoch 515, loss = 0.0015053291121478421\n",
      "Epoch 516, loss = 0.0014992895646209333\n",
      "Epoch 517, loss = 0.0014932876426023503\n",
      "Epoch 518, loss = 0.0014873230239678449\n",
      "Epoch 519, loss = 0.0014813953901168162\n",
      "Epoch 520, loss = 0.0014755044259252795\n",
      "Epoch 521, loss = 0.0014696498196998255\n",
      "Epoch 522, loss = 0.0014638312631314546\n",
      "Epoch 523, loss = 0.0014580484512510827\n",
      "Epoch 524, loss = 0.001452301082385613\n",
      "Epoch 525, loss = 0.0014465888581141016\n",
      "Epoch 526, loss = 0.001440911483225155\n",
      "Epoch 527, loss = 0.0014352686656746462\n",
      "Epoch 528, loss = 0.0014296601165446397\n",
      "Epoch 529, loss = 0.0014240855500021567\n",
      "Epoch 530, loss = 0.0014185446832592052\n",
      "Epoch 531, loss = 0.0014130372365333138\n",
      "Epoch 532, loss = 0.0014075629330081258\n",
      "Epoch 533, loss = 0.0014021214987956132\n",
      "Epoch 534, loss = 0.001396712662897791\n",
      "Epoch 535, loss = 0.001391336157170197\n",
      "Epoch 536, loss = 0.0013859917162839543\n",
      "Epoch 537, loss = 0.0013806790776915712\n",
      "Epoch 538, loss = 0.0013753979815896435\n",
      "Epoch 539, loss = 0.0013701481708848129\n",
      "Epoch 540, loss = 0.0013649293911591406\n",
      "Epoch 541, loss = 0.0013597413906360986\n",
      "Epoch 542, loss = 0.0013545839201470225\n",
      "Epoch 543, loss = 0.0013494567330984053\n",
      "Epoch 544, loss = 0.0013443595854391772\n",
      "Epoch 545, loss = 0.0013392922356290967\n",
      "Epoch 546, loss = 0.0013342544446067431\n",
      "Epoch 547, loss = 0.0013292459757589343\n",
      "Epoch 548, loss = 0.0013242665948900955\n",
      "Epoch 549, loss = 0.0013193160701919992\n",
      "Epoch 550, loss = 0.001314394172214253\n",
      "Epoch 551, loss = 0.0013095006738346197\n",
      "Epoch 552, loss = 0.0013046353502310303\n",
      "Epoch 553, loss = 0.001299797978852339\n",
      "Epoch 554, loss = 0.0012949883393912085\n",
      "Epoch 555, loss = 0.0012902062137554859\n",
      "Epoch 556, loss = 0.0012854513860420947\n",
      "Epoch 557, loss = 0.0012807236425094687\n",
      "Epoch 558, loss = 0.0012760227715518239\n",
      "Epoch 559, loss = 0.0012713485636725776\n",
      "Epoch 560, loss = 0.001266700811458849\n",
      "Epoch 561, loss = 0.0012620793095568668\n",
      "Epoch 562, loss = 0.0012574838546457864\n",
      "Epoch 563, loss = 0.001252914245414402\n",
      "Epoch 564, loss = 0.0012483702825362103\n",
      "Epoch 565, loss = 0.001243851768645829\n",
      "Epoch 566, loss = 0.0012393585083152524\n",
      "Epoch 567, loss = 0.0012348903080311446\n",
      "Epoch 568, loss = 0.0012304469761714157\n",
      "Epoch 569, loss = 0.0012260283229829994\n",
      "Epoch 570, loss = 0.0012216341605599172\n",
      "Epoch 571, loss = 0.0012172643028206152\n",
      "Epoch 572, loss = 0.0012129185654870317\n",
      "Epoch 573, loss = 0.0012085967660630712\n",
      "Epoch 574, loss = 0.0012042987238137467\n",
      "Epoch 575, loss = 0.0012000242597440857\n",
      "Epoch 576, loss = 0.0011957731965792767\n",
      "Epoch 577, loss = 0.0011915453587439675\n",
      "Epoch 578, loss = 0.0011873405723429346\n",
      "Epoch 579, loss = 0.001183158665140996\n",
      "Epoch 580, loss = 0.0011789994665442956\n",
      "Epoch 581, loss = 0.001174862807580628\n",
      "Epoch 582, loss = 0.001170748520880874\n",
      "Epoch 583, loss = 0.0011666564406609392\n",
      "Epoch 584, loss = 0.0011625864027026759\n",
      "Epoch 585, loss = 0.0011585382443365657\n",
      "Epoch 586, loss = 0.0011545118044233662\n",
      "Epoch 587, loss = 0.0011505069233370018\n",
      "Epoch 588, loss = 0.0011465234429470451\n",
      "Epoch 589, loss = 0.0011425612066017043\n",
      "Epoch 590, loss = 0.001138620059111051\n",
      "Epoch 591, loss = 0.0011346998467299696\n",
      "Epoch 592, loss = 0.0011308004171427047\n",
      "Epoch 593, loss = 0.0011269216194457214\n",
      "Epoch 594, loss = 0.0011230633041321247\n",
      "Epoch 595, loss = 0.0011192253230761916\n",
      "Epoch 596, loss = 0.0011154075295172524\n",
      "Epoch 597, loss = 0.0011116097780449325\n",
      "Epoch 598, loss = 0.0011078319245837766\n",
      "Epoch 599, loss = 0.0011040738263783248\n",
      "Epoch 600, loss = 0.001100335341978075\n",
      "Epoch 601, loss = 0.001096616331223781\n",
      "Epoch 602, loss = 0.00109291665523242\n",
      "Epoch 603, loss = 0.0010892361763828438\n",
      "Epoch 604, loss = 0.0010855747583027482\n",
      "Epoch 605, loss = 0.0010819322658540877\n",
      "Epoch 606, loss = 0.0010783085651195625\n",
      "Epoch 607, loss = 0.0010747035233894826\n",
      "Epoch 608, loss = 0.0010711170091485158\n",
      "Epoch 609, loss = 0.0010675488920620718\n",
      "Epoch 610, loss = 0.0010639990429641268\n",
      "Epoch 611, loss = 0.001060467333843831\n",
      "Epoch 612, loss = 0.0010569536378334304\n",
      "Epoch 613, loss = 0.0010534578291955395\n",
      "Epoch 614, loss = 0.0010499797833109189\n",
      "Epoch 615, loss = 0.0010465193766664176\n",
      "Epoch 616, loss = 0.00104307648684312\n",
      "Epoch 617, loss = 0.0010396509925043858\n",
      "Epoch 618, loss = 0.0010362427733842368\n",
      "Epoch 619, loss = 0.001032851710275863\n",
      "Epoch 620, loss = 0.0010294776850204937\n",
      "Epoch 621, loss = 0.0010261205804956615\n",
      "Epoch 622, loss = 0.0010227802806046335\n",
      "Epoch 623, loss = 0.0010194566702652317\n",
      "Epoch 624, loss = 0.0010161496353992283\n",
      "Epoch 625, loss = 0.0010128590629214641\n",
      "Epoch 626, loss = 0.001009584840729394\n",
      "Epoch 627, loss = 0.0010063268576928368\n",
      "Epoch 628, loss = 0.001003085003643502\n",
      "Epoch 629, loss = 0.000999859169364929\n",
      "Epoch 630, loss = 0.0009966492465828098\n",
      "Epoch 631, loss = 0.0009934551279544286\n",
      "Epoch 632, loss = 0.0009902767070593596\n",
      "Epoch 633, loss = 0.000987113878389887\n",
      "Epoch 634, loss = 0.0009839665373411678\n",
      "Epoch 635, loss = 0.0009808345802020965\n",
      "Epoch 636, loss = 0.000977717904145935\n",
      "Epoch 637, loss = 0.0009746164072208791\n",
      "Epoch 638, loss = 0.0009715299883417093\n",
      "Epoch 639, loss = 0.0009684585472796943\n",
      "Epoch 640, loss = 0.0009654019846551091\n",
      "Epoch 641, loss = 0.0009623602019271429\n",
      "Epoch 642, loss = 0.0009593331013864172\n",
      "Epoch 643, loss = 0.0009563205861457093\n",
      "Epoch 644, loss = 0.0009533225601318808\n",
      "Epoch 645, loss = 0.0009503389280774887\n",
      "Epoch 646, loss = 0.0009473695955124197\n",
      "Epoch 647, loss = 0.0009444144687561148\n",
      "Epoch 648, loss = 0.0009414734549089619\n",
      "Epoch 649, loss = 0.0009385464618450963\n",
      "Epoch 650, loss = 0.000935633398204017\n",
      "Epoch 651, loss = 0.0009327341733830914\n",
      "Epoch 652, loss = 0.0009298486975297094\n",
      "Epoch 653, loss = 0.0009269768815340865\n",
      "Epoch 654, loss = 0.0009241186370214366\n",
      "Epoch 655, loss = 0.0009212738763447011\n",
      "Epoch 656, loss = 0.0009184425125772611\n",
      "Epoch 657, loss = 0.0009156244595060377\n",
      "Epoch 658, loss = 0.0009128196316237602\n",
      "Epoch 659, loss = 0.0009100279441225657\n",
      "Epoch 660, loss = 0.0009072493128864014\n",
      "Epoch 661, loss = 0.0009044836544848444\n",
      "Epoch 662, loss = 0.0009017308861657461\n",
      "Epoch 663, loss = 0.0008989909258488507\n",
      "Epoch 664, loss = 0.0008962636921191408\n",
      "Epoch 665, loss = 0.0008935491042197391\n",
      "Epoch 666, loss = 0.0008908470820465094\n",
      "Epoch 667, loss = 0.0008881575461406284\n",
      "Epoch 668, loss = 0.0008854804176827804\n",
      "Epoch 669, loss = 0.000882815618486746\n",
      "Epoch 670, loss = 0.0008801630709931559\n",
      "Epoch 671, loss = 0.0008775226982638188\n",
      "Epoch 672, loss = 0.000874894423974699\n",
      "Epoch 673, loss = 0.0008722781724111346\n",
      "Epoch 674, loss = 0.0008696738684613142\n",
      "Epoch 675, loss = 0.0008670814376099604\n",
      "Epoch 676, loss = 0.0008645008059337137\n",
      "Epoch 677, loss = 0.0008619319000945199\n",
      "Epoch 678, loss = 0.0008593746473341081\n",
      "Epoch 679, loss = 0.0008568289754690263\n",
      "Epoch 680, loss = 0.0008542948128842321\n",
      "Epoch 681, loss = 0.0008517720885283319\n",
      "Epoch 682, loss = 0.0008492607319078932\n",
      "Epoch 683, loss = 0.000846760673082057\n",
      "Epoch 684, loss = 0.0008442718426576273\n",
      "Epoch 685, loss = 0.0008417941717832762\n",
      "Epoch 686, loss = 0.0008393275921449696\n",
      "Epoch 687, loss = 0.0008368720359606136\n",
      "Epoch 688, loss = 0.0008344274359750204\n",
      "Epoch 689, loss = 0.0008319937254548889\n",
      "Epoch 690, loss = 0.0008295708381839728\n",
      "Epoch 691, loss = 0.0008271587084581738\n",
      "Epoch 692, loss = 0.0008247572710806927\n",
      "Epoch 693, loss = 0.0008223664613573994\n",
      "Epoch 694, loss = 0.0008199862150919847\n",
      "Epoch 695, loss = 0.0008176164685811841\n",
      "Epoch 696, loss = 0.0008152571586106136\n",
      "Epoch 697, loss = 0.000812908222449679\n",
      "Epoch 698, loss = 0.0008105695978475247\n",
      "Epoch 699, loss = 0.000808241223028017\n",
      "Epoch 700, loss = 0.0008059230366863596\n",
      "Epoch 701, loss = 0.0008036149779832071\n",
      "Epoch 702, loss = 0.0008013169865419847\n",
      "Epoch 703, loss = 0.0007990290024433173\n",
      "Epoch 704, loss = 0.0007967509662216821\n",
      "Epoch 705, loss = 0.0007944828188607094\n",
      "Epoch 706, loss = 0.0007922245017892615\n",
      "Epoch 707, loss = 0.0007899759568775108\n",
      "Epoch 708, loss = 0.0007877371264324263\n",
      "Epoch 709, loss = 0.0007855079531942805\n",
      "Epoch 710, loss = 0.0007832883803326153\n",
      "Epoch 711, loss = 0.0007810783514419152\n",
      "Epoch 712, loss = 0.000778877810538114\n",
      "Epoch 713, loss = 0.000776686702054859\n",
      "Epoch 714, loss = 0.0007745049708394792\n",
      "Epoch 715, loss = 0.0007723325621491417\n",
      "Epoch 716, loss = 0.0007701694216475237\n",
      "Epoch 717, loss = 0.0007680154954009668\n",
      "Epoch 718, loss = 0.000765870729874689\n",
      "Epoch 719, loss = 0.0007637350719294375\n",
      "Epoch 720, loss = 0.0007616084688178873\n",
      "Epoch 721, loss = 0.0007594908681808317\n",
      "Epoch 722, loss = 0.0007573822180442646\n",
      "Epoch 723, loss = 0.0007552824668154807\n",
      "Epoch 724, loss = 0.0007531915632797678\n",
      "Epoch 725, loss = 0.0007511094565974713\n",
      "Epoch 726, loss = 0.0007490360962996119\n",
      "Epoch 727, loss = 0.0007469714322859696\n",
      "Epoch 728, loss = 0.000744915414820609\n",
      "Epoch 729, loss = 0.0007428679945294994\n",
      "Epoch 730, loss = 0.0007408291223968568\n",
      "Epoch 731, loss = 0.0007387987497620973\n",
      "Epoch 732, loss = 0.000736776828316679\n",
      "Epoch 733, loss = 0.0007347633101011849\n",
      "Epoch 734, loss = 0.0007327581475019061\n",
      "Epoch 735, loss = 0.0007307612932485739\n",
      "Epoch 736, loss = 0.0007287727004098875\n",
      "Epoch 737, loss = 0.0007267923223923555\n",
      "Epoch 738, loss = 0.0007248201129360117\n",
      "Epoch 739, loss = 0.000722856026112015\n",
      "Epoch 740, loss = 0.0007209000163198093\n",
      "Epoch 741, loss = 0.0007189520382842432\n",
      "Epoch 742, loss = 0.0007170120470525813\n",
      "Epoch 743, loss = 0.0007150799979917721\n",
      "Epoch 744, loss = 0.0007131558467858306\n",
      "Epoch 745, loss = 0.0007112395494330144\n",
      "Epoch 746, loss = 0.0007093310622432039\n",
      "Epoch 747, loss = 0.0007074303418348776\n",
      "Epoch 748, loss = 0.0007055373451328806\n",
      "Epoch 749, loss = 0.0007036520293657143\n",
      "Epoch 750, loss = 0.0007017743520625654\n",
      "Epoch 751, loss = 0.0006999042710513904\n",
      "Epoch 752, loss = 0.0006980417444557434\n",
      "Epoch 753, loss = 0.0006961867306926372\n",
      "Epoch 754, loss = 0.0006943391884697411\n",
      "Epoch 755, loss = 0.0006924990767832806\n",
      "Epoch 756, loss = 0.0006906663549153462\n",
      "Epoch 757, loss = 0.0006888409824316073\n",
      "Epoch 758, loss = 0.0006870229191786028\n",
      "Epoch 759, loss = 0.0006852121252818639\n",
      "Epoch 760, loss = 0.0006834085611429995\n",
      "Epoch 761, loss = 0.0006816121874379867\n",
      "Epoch 762, loss = 0.0006798229651143306\n",
      "Epoch 763, loss = 0.0006780408553890213\n",
      "Epoch 764, loss = 0.0006762658197464528\n",
      "Epoch 765, loss = 0.0006744978199357696\n",
      "Epoch 766, loss = 0.0006727368179689155\n",
      "Epoch 767, loss = 0.0006709827761184241\n",
      "Epoch 768, loss = 0.0006692356569151539\n",
      "Epoch 769, loss = 0.0006674954231463743\n",
      "Epoch 770, loss = 0.0006657620378532972\n",
      "Epoch 771, loss = 0.000664035464329276\n",
      "Epoch 772, loss = 0.0006623156661173719\n",
      "Epoch 773, loss = 0.0006606026070088132\n",
      "Epoch 774, loss = 0.0006588962510402667\n",
      "Epoch 775, loss = 0.0006571965624924992\n",
      "Epoch 776, loss = 0.0006555035058879091\n",
      "Epoch 777, loss = 0.0006538170459887248\n",
      "Epoch 778, loss = 0.0006521371477948144\n",
      "Epoch 779, loss = 0.0006504637765421621\n",
      "Epoch 780, loss = 0.0006487968977005291\n",
      "Epoch 781, loss = 0.0006471364769717834\n",
      "Epoch 782, loss = 0.0006454824802879488\n",
      "Epoch 783, loss = 0.0006438348738091083\n",
      "Epoch 784, loss = 0.0006421936239219547\n",
      "Epoch 785, loss = 0.0006405586972377671\n",
      "Epoch 786, loss = 0.0006389300605903329\n",
      "Epoch 787, loss = 0.0006373076810345341\n",
      "Epoch 788, loss = 0.000635691525844456\n",
      "Epoch 789, loss = 0.000634081562511251\n",
      "Epoch 790, loss = 0.0006324777587422279\n",
      "Epoch 791, loss = 0.0006308800824580516\n",
      "Epoch 792, loss = 0.0006292885017918482\n",
      "Epoch 793, loss = 0.0006277029850869989\n",
      "Epoch 794, loss = 0.0006261235008960412\n",
      "Epoch 795, loss = 0.0006245500179780375\n",
      "Epoch 796, loss = 0.0006229825052981058\n",
      "Epoch 797, loss = 0.0006214209320245828\n",
      "Epoch 798, loss = 0.0006198652675285952\n",
      "Epoch 799, loss = 0.0006183154813814061\n",
      "Epoch 800, loss = 0.0006167715433532079\n",
      "Epoch 801, loss = 0.000615233423411915\n",
      "Epoch 802, loss = 0.0006137010917211224\n",
      "Epoch 803, loss = 0.0006121745186386391\n",
      "Epoch 804, loss = 0.0006106536747150037\n",
      "Epoch 805, loss = 0.0006091385306922566\n",
      "Epoch 806, loss = 0.0006076290575017348\n",
      "Epoch 807, loss = 0.0006061252262633445\n",
      "Epoch 808, loss = 0.0006046270082834291\n",
      "Epoch 809, loss = 0.0006031343750538943\n",
      "Epoch 810, loss = 0.0006016472982502421\n",
      "Epoch 811, loss = 0.0006001657497305119\n",
      "Epoch 812, loss = 0.0005986897015336847\n",
      "Epoch 813, loss = 0.0005972191258781421\n",
      "Epoch 814, loss = 0.000595753995160719\n",
      "Epoch 815, loss = 0.0005942942819544783\n",
      "Epoch 816, loss = 0.0005928399590085775\n",
      "Epoch 817, loss = 0.0005913909992455439\n",
      "Epoch 818, loss = 0.0005899473757609931\n",
      "Epoch 819, loss = 0.0005885090618216076\n",
      "Epoch 820, loss = 0.0005870760308643381\n",
      "Epoch 821, loss = 0.0005856482564945844\n",
      "Epoch 822, loss = 0.0005842257124853971\n",
      "Epoch 823, loss = 0.0005828083727755856\n",
      "Epoch 824, loss = 0.0005813962114693255\n",
      "Epoch 825, loss = 0.0005799892028337685\n",
      "Epoch 826, loss = 0.000578587321298778\n",
      "Epoch 827, loss = 0.0005771905414552809\n",
      "Epoch 828, loss = 0.0005757988380538174\n",
      "Epoch 829, loss = 0.0005744121860034851\n",
      "Epoch 830, loss = 0.0005730305603712504\n",
      "Epoch 831, loss = 0.0005716539363798335\n",
      "Epoch 832, loss = 0.0005702822894073374\n",
      "Epoch 833, loss = 0.0005689155949852438\n",
      "Epoch 834, loss = 0.0005675538287981704\n",
      "Epoch 835, loss = 0.0005661969666821276\n",
      "Epoch 836, loss = 0.0005648449846231637\n",
      "Epoch 837, loss = 0.0005634978587571038\n",
      "Epoch 838, loss = 0.0005621555653676578\n",
      "Epoch 839, loss = 0.0005608180808853075\n",
      "Epoch 840, loss = 0.0005594853818868037\n",
      "Epoch 841, loss = 0.000558157445093368\n",
      "Epoch 842, loss = 0.0005568342473702451\n",
      "Epoch 843, loss = 0.0005555157657250167\n",
      "Epoch 844, loss = 0.0005542019773071533\n",
      "Epoch 845, loss = 0.00055289285940618\n",
      "Epoch 846, loss = 0.0005515883894514508\n",
      "Epoch 847, loss = 0.0005502885450107214\n",
      "Epoch 848, loss = 0.0005489933037888685\n",
      "Epoch 849, loss = 0.0005477026436274626\n",
      "Epoch 850, loss = 0.0005464165425032648\n",
      "Epoch 851, loss = 0.0005451349785274296\n",
      "Epoch 852, loss = 0.0005438579299443348\n",
      "Epoch 853, loss = 0.0005425853751308215\n",
      "Epoch 854, loss = 0.0005413172925953949\n",
      "Epoch 855, loss = 0.000540053660976408\n",
      "Epoch 856, loss = 0.0005387944590422442\n",
      "Epoch 857, loss = 0.000537539665689446\n",
      "Epoch 858, loss = 0.0005362892599421745\n",
      "Epoch 859, loss = 0.0005350432209514681\n",
      "Epoch 860, loss = 0.0005338015279938146\n",
      "Epoch 861, loss = 0.0005325641604705556\n",
      "Epoch 862, loss = 0.0005313310979067933\n",
      "Epoch 863, loss = 0.0005301023199508693\n",
      "Epoch 864, loss = 0.0005288778063730118\n",
      "Epoch 865, loss = 0.0005276575370645925\n",
      "Epoch 866, loss = 0.000526441492037328\n",
      "Epoch 867, loss = 0.0005252296514227239\n",
      "Epoch 868, loss = 0.0005240219954701648\n",
      "Epoch 869, loss = 0.0005228185045472086\n",
      "Epoch 870, loss = 0.0005216191591381041\n",
      "Epoch 871, loss = 0.0005204239398433077\n",
      "Epoch 872, loss = 0.0005192328273782401\n",
      "Epoch 873, loss = 0.0005180458025726566\n",
      "Epoch 874, loss = 0.0005168628463699776\n",
      "Epoch 875, loss = 0.0005156839398262689\n",
      "Epoch 876, loss = 0.0005145090641094811\n",
      "Epoch 877, loss = 0.0005133382004986698\n",
      "Epoch 878, loss = 0.0005121713303831239\n",
      "Epoch 879, loss = 0.0005110084352618092\n",
      "Epoch 880, loss = 0.0005098494967421803\n",
      "Epoch 881, loss = 0.0005086944965398102\n",
      "Epoch 882, loss = 0.0005075434164777025\n",
      "Epoch 883, loss = 0.0005063962384845504\n",
      "Epoch 884, loss = 0.000505252944595345\n",
      "Epoch 885, loss = 0.0005041135169500044\n",
      "Epoch 886, loss = 0.0005029779377922428\n",
      "Epoch 887, loss = 0.0005018461894694577\n",
      "Epoch 888, loss = 0.0005007182544316729\n",
      "Epoch 889, loss = 0.000499594115231037\n",
      "Epoch 890, loss = 0.0004984737545206374\n",
      "Epoch 891, loss = 0.0004973571550544617\n",
      "Epoch 892, loss = 0.0004962442996861555\n",
      "Epoch 893, loss = 0.0004951351713684288\n",
      "Epoch 894, loss = 0.0004940297531527584\n",
      "Epoch 895, loss = 0.0004929280281880156\n",
      "Epoch 896, loss = 0.000491829979720429\n",
      "Epoch 897, loss = 0.0004907355910923235\n",
      "Epoch 898, loss = 0.000489644845741915\n",
      "Epoch 899, loss = 0.0004885577272024953\n",
      "Epoch 900, loss = 0.00048747421910178153\n",
      "Epoch 901, loss = 0.0004863943051610458\n",
      "Epoch 902, loss = 0.00048531796919479866\n",
      "Epoch 903, loss = 0.0004842451951098804\n",
      "Epoch 904, loss = 0.00048317596690510825\n",
      "Epoch 905, loss = 0.00048211026867033105\n",
      "Epoch 906, loss = 0.0004810480845857797\n",
      "Epoch 907, loss = 0.0004799893989218623\n",
      "Epoch 908, loss = 0.0004789341960381448\n",
      "Epoch 909, loss = 0.00047788246038299757\n",
      "Epoch 910, loss = 0.0004768341764926139\n",
      "Epoch 911, loss = 0.00047578932899069293\n",
      "Epoch 912, loss = 0.0004747479025880134\n",
      "Epoch 913, loss = 0.0004737098820814508\n",
      "Epoch 914, loss = 0.0004726752523535509\n",
      "Epoch 915, loss = 0.0004716439983720096\n",
      "Epoch 916, loss = 0.0004706161051888393\n",
      "Epoch 917, loss = 0.0004695915579402928\n",
      "Epoch 918, loss = 0.00046857034184567863\n",
      "Epoch 919, loss = 0.0004675524422073402\n",
      "Epoch 920, loss = 0.0004665378444097294\n",
      "Epoch 921, loss = 0.0004655265339189064\n",
      "Epoch 922, loss = 0.00046451849628229664\n",
      "Epoch 923, loss = 0.00046351371712754255\n",
      "Epoch 924, loss = 0.0004625121821625398\n",
      "Epoch 925, loss = 0.0004615138771745844\n",
      "Epoch 926, loss = 0.0004605187880299453\n",
      "Epoch 927, loss = 0.00045952690067340156\n",
      "Epoch 928, loss = 0.00045853820112737043\n",
      "Epoch 929, loss = 0.00045755267549190635\n",
      "Epoch 930, loss = 0.00045657030994379356\n",
      "Epoch 931, loss = 0.00045559109073604437\n",
      "Epoch 932, loss = 0.00045461500419767656\n",
      "Epoch 933, loss = 0.0004536420367327494\n",
      "Epoch 934, loss = 0.0004526721748205667\n",
      "Epoch 935, loss = 0.0004517054050142684\n",
      "Epoch 936, loss = 0.00045074171394108914\n",
      "Epoch 937, loss = 0.0004497810883014319\n",
      "Epoch 938, loss = 0.00044882351486868116\n",
      "Epoch 939, loss = 0.00044786898048855485\n",
      "Epoch 940, loss = 0.00044691747207854766\n",
      "Epoch 941, loss = 0.0004459689766274119\n",
      "Epoch 942, loss = 0.00044502348119528594\n",
      "Epoch 943, loss = 0.0004440809729122312\n",
      "Epoch 944, loss = 0.00044314143897865695\n",
      "Epoch 945, loss = 0.0004422048666643011\n",
      "Epoch 946, loss = 0.0004412712433081737\n",
      "Epoch 947, loss = 0.00044034055631764956\n",
      "Epoch 948, loss = 0.0004394127931682446\n",
      "Epoch 949, loss = 0.00043848794140374504\n",
      "Epoch 950, loss = 0.0004375659886342808\n",
      "Epoch 951, loss = 0.0004366469225377137\n",
      "Epoch 952, loss = 0.00043573073085776656\n",
      "Epoch 953, loss = 0.00043481740140424474\n",
      "Epoch 954, loss = 0.0004339069220525347\n",
      "Epoch 955, loss = 0.0004329992807430123\n",
      "Epoch 956, loss = 0.00043209446548083714\n",
      "Epoch 957, loss = 0.00043119246433545225\n",
      "Epoch 958, loss = 0.0004302932654401028\n",
      "Epoch 959, loss = 0.0004293968569913339\n",
      "Epoch 960, loss = 0.00042850322724880636\n",
      "Epoch 961, loss = 0.0004276123645349061\n",
      "Epoch 962, loss = 0.0004267242572339668\n",
      "Epoch 963, loss = 0.0004258388937923789\n",
      "Epoch 964, loss = 0.00042495626271773904\n",
      "Epoch 965, loss = 0.0004240763525787923\n",
      "Epoch 966, loss = 0.0004231991520047288\n",
      "Epoch 967, loss = 0.0004223246496852572\n",
      "Epoch 968, loss = 0.00042145283436947407\n",
      "Epoch 969, loss = 0.000420583694866345\n",
      "Epoch 970, loss = 0.0004197172200436858\n",
      "Epoch 971, loss = 0.0004188533988279577\n",
      "Epoch 972, loss = 0.0004179922202043232\n",
      "Epoch 973, loss = 0.0004171336732154038\n",
      "Epoch 974, loss = 0.00041627774696165036\n",
      "Epoch 975, loss = 0.00041542443060062093\n",
      "Epoch 976, loss = 0.00041457371334701585\n",
      "Epoch 977, loss = 0.0004137255844714192\n",
      "Epoch 978, loss = 0.0004128800333011499\n",
      "Epoch 979, loss = 0.0004120370492189461\n",
      "Epoch 980, loss = 0.0004111966216630758\n",
      "Epoch 981, loss = 0.0004103587401268734\n",
      "Epoch 982, loss = 0.00040952339415844364\n",
      "Epoch 983, loss = 0.00040869057336008594\n",
      "Epoch 984, loss = 0.00040786026738831317\n",
      "Epoch 985, loss = 0.0004070324659531471\n",
      "Epoch 986, loss = 0.0004062071588181369\n",
      "Epoch 987, loss = 0.0004053843357998395\n",
      "Epoch 988, loss = 0.0004045639867673199\n",
      "Epoch 989, loss = 0.00040374610164213174\n",
      "Epoch 990, loss = 0.00040293067039787227\n",
      "Epoch 991, loss = 0.00040211768305979326\n",
      "Epoch 992, loss = 0.00040130712970428247\n",
      "Epoch 993, loss = 0.00040049900045939914\n",
      "Epoch 994, loss = 0.0003996932855031711\n",
      "Epoch 995, loss = 0.0003988899750647414\n",
      "Epoch 996, loss = 0.0003980890594228505\n",
      "Epoch 997, loss = 0.00039729052890639043\n",
      "Epoch 998, loss = 0.000396494373893386\n",
      "Epoch 999, loss = 0.0003957005848114212\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary of weights\n",
    "W = {\n",
    "     1: torch.nn.Parameter(torch.randn(5, 4, dtype = torch.float64)*0.01),\n",
    "     2: torch.nn.Parameter(torch.randn(4, 3, dtype = torch.float64)*0.01) \n",
    "     }\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(W.items(), lr = 1e-02)\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(X): # for which part of the data are we calculating the loss\n",
    "  # Raw scores for layer-1\n",
    "  Z1 = X @ W[1]\n",
    "\n",
    "  # ReLU-activated scores for layer-1\n",
    "  A1 = ReLU(Z1)\n",
    "\n",
    "  # Raw scores for layer-2\n",
    "  Z2 = A1 @ W[2]\n",
    "\n",
    "  # Softmax-activated scores for layer-2\n",
    "  softmax = torch.nn.Softmax(dim = 1)\n",
    "  A2 = softmax(Z2)\n",
    "\n",
    "  # Calculate the average training loss\n",
    "  L = torch.mean(-torch.log(torch.sum(Y*A2, dim = 1)))\n",
    "  return L\n",
    "\n",
    "# Optimization loop\n",
    "num_epochs = 1000\n",
    "loss_train = np.empty(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "  # Zero out the gradients\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Forward propagation (loss calculation)\n",
    "  loss = loss_fn(X_std)\n",
    "\n",
    "  # Backward propagation and optimization\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Print the loss every epoch\n",
    "  loss_train[epoch] = loss.item()\n",
    "  print(f'Epoch {epoch}, loss = {loss_train[epoch]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Plot training loss curve\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot train loss as a function of epoch:\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "fig.tight_layout(pad = 4.0)\n",
    "ax.plot(loss_train, 'b')\n",
    "ax.set_xlabel('Epoch', fontsize = 12)\n",
    "ax.set_ylabel('Loss value', fontsize = 12)\n",
    "ax.set_title('Training Loss vs. Epoch', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "How good are the trained weights when applied to the training data?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels for training data: tensor([1, 0, 1, 2, 0, 2])\n",
      "True output labels: tensor([1, 0, 1, 2, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  # Raw scores for layer-1\n",
    "  Z1 = torch.matmul(X_std, W[1])\n",
    "\n",
    "  # ReLU-activated scores for layer-1\n",
    "  A1 = ReLU(Z1)\n",
    "\n",
    "  # Raw scores for layer-2\n",
    "  Z2 = torch.matmul(A1, W[2])\n",
    "\n",
    "  # Softmax-activated scores for layer-2\n",
    "  softmax = torch.nn.Softmax(dim = 1)\n",
    "  A2 = softmax(Z2)\n",
    "\n",
    "# Get predicted labels \n",
    "print(f'Predicted labels for training data: {A2.argmax(dim = 1)}')\n",
    "\n",
    "# True output labels\n",
    "print(f'True output labels: {Y.argmax(dim = 1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Loading the Bengaluru Hosueprice dataset and performing a train-test split\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load housing data\n",
    "file = DATA_DIR+'houseprices_cleaned.csv'\n",
    "df= pd.read_csv(file, header = 0).dropna()\n",
    "\n",
    "## Train and test split of the data\n",
    "X = df[['area', 'rent']]\n",
    "y = df['price_per_sqft']\n",
    "X_train, X_test, Y_train, Y_test = ?\n",
    "\n",
    "# Standardize data\n",
    "sc = StandardScaler()\n",
    "X_train = ?\n",
    "X_test = ?\n",
    "\n",
    "# Convert train and test data to numpy arrays (note that Y should be a 1-column matrix)\n",
    "X_train = torch.tensor(X_train, dtype = torch.float64)\n",
    "X_test = torch.tensor(X_test, dtype = torch.float64)\n",
    "Y_train = torch.tensor(Y_train.values, dtype = torch.float32).reshape(-1, 1)\n",
    "Y_test = torch.tensor(Y_test.values, dtype = torch.float32).reshape(-1, 1)\n",
    "\n",
    "num_samples = X_train.shape[0]\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "print('Housing data set')\n",
    "print('---------------------')\n",
    "print('Number of training samples = %d'%(num_samples))\n",
    "print('Number of features = %d'%(num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Using PyTorch, calculate the optimal weights $\\mathbf{W}^{[1]}$ and $\\mathbf{W}^{[2]}$ for the 2-layer neural network\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of weights\n",
    "W = {\n",
    "     '1': torch.nn.Parameter(torch.randn(?, ?, dtype = torch.float64) * 0.01),\n",
    "     '2': torch.nn.Parameter(torch.randn(?, ?, dtype = torch.float64) * 0.01)\n",
    "     }\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(W.items(), lr = 1e-02)\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(X, Y):\n",
    "  # Raw scores for layer-1\n",
    "  Z1 = torch.matmul(X, W['1'])\n",
    "\n",
    "  # ReLU-activated scores for layer-1\n",
    "  A1 = ReLU(Z1)\n",
    "\n",
    "  # Raw scores for layer-2\n",
    "  Z2 = torch.matmul(A1, W['2'])\n",
    "\n",
    "  # Calculate the average training loss\n",
    "  L = ?\n",
    "  return L\n",
    "\n",
    "# Optimization loop\n",
    "num_epochs = 50000\n",
    "loss_train = np.empty(num_epochs)\n",
    "loss_test = np.empty(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "  # Zero out the gradients\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Forward propagation (loss calculation)\n",
    "  loss = loss_fn(?, ?)   \n",
    "\n",
    "  # Backward propagation and optimization\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Print the loss every 1000 epochs\n",
    "  loss_train[epoch] = loss.item()\n",
    "  # Test loss\n",
    "  with torch.no_grad():\n",
    "    loss = loss_fn(?, ?)\n",
    "    loss_test[epoch] = loss.item()\n",
    "  if epoch % 1000 == 0:\n",
    "    print(f'Epoch {epoch}, train loss = {loss_train[epoch]}, test loss = {loss_test[epoch]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Plot train and test loss curves\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot train and test loss as a function of epoch:\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "fig.tight_layout(pad = 4.0)\n",
    "ax.plot(loss_train, 'b', label = 'Train')\n",
    "ax.plot(loss_test, 'r', label = 'Test')\n",
    "ax.set_xlabel('Epoch', fontsize = 12)\n",
    "ax.set_ylabel('Loss value', fontsize = 12)\n",
    "ax.legend()\n",
    "ax.set_title('Loss vs. Epoch', fontsize = 14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "How good are the trained weights when applied to the test data?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  # Raw scores for layer-1\n",
    "  Z1 = torch.matmul(?, ?)\n",
    "\n",
    "  # ReLU-activated scores for layer-1\n",
    "  A1 = ReLU(?)\n",
    "\n",
    "  # Raw scores for layer-2\n",
    "  Z2 = ?      \n",
    "\n",
    "print(f'Test data true and predicted house prices {torch.hstack([?, ?])}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
