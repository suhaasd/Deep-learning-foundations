{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7g7bxFCHxGP"
   },
   "source": [
    "$$\\Large\\boxed{\\text{AME 5202 Deep Learning, Even Semester 2026}}$$\n",
    "\n",
    "$$\\large\\text{Theme}: \\underline{\\text{computational foundations of gradient}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDK9fC6uiBGE"
   },
   "source": [
    "---\n",
    "\n",
    "Load essential libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1759082932391,
     "user": {
      "displayName": "S N S Acharya",
      "userId": "14786945180387920086"
     },
     "user_tz": -330
    },
    "id": "20W0d4ruQjE4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfYXkqmLiVLM"
   },
   "source": [
    "---\n",
    "\n",
    "Mount Google Drive folder if running Google Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYzBBBxqiaGa"
   },
   "outputs": [],
   "source": [
    "## Mount Google drive folder if running in Colab\n",
    "if('google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/Office of Online Education/MDS6304_Webinar_October2025'\n",
    "    DATA_DIR = DIR+'/Data/'\n",
    "else:\n",
    "    DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Automatic differentiation in PyTorch.\n",
    "\n",
    "Example: calculate the sensitivity of $L(w) = 4w+w^3$ w.r.t. the input $w$ at $w=1.$\n",
    "\n",
    "Sensitivity $\\nabla_wL = 4+3w^2,$ which at $w=1$ is equal to $4+3\\times1^2=7.$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create linspace and define function L(w)\n",
    "w_vals = torch.linspace(-2, 2, 1000)\n",
    "L_fn = lambda w: 4 * w + w ** 3\n",
    "\n",
    "# 2. Plot L(w)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.plot(w_vals, L_fn(w_vals))\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('L')\n",
    "ax.axhline(y = 0, color = 'white')\n",
    "ax.axvline(x = 0, color = 'white')\n",
    "\n",
    "# Mark L(1)\n",
    "ax.set_title('L(w) = 4w+w^3 at w=1')\n",
    "ax.scatter(1, L_fn(torch.tensor(1.0)), c = 'red')\n",
    "ax.scatter(1, 0, c='red')\n",
    "plt.show()\n",
    "\n",
    "# 3. Compute gradient of L at w = 1\n",
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "L = 4 * w + w ** 3\n",
    "L.backward()  # Compute gradient a.k.a sensitivity\n",
    "print('The gradient of L w.r.t. w at w = 1 is %f' % w.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Another example with a negative gradient.\n",
    "\n",
    "Calculate the sensitivity of $L(w) = 4w-w^3$ w.r.t. the input $w$ at $w=1.5.$\n",
    "\n",
    "Sensitivity $\\nabla_wL = 4-3w^2,$ which at $w=1.5$ is equal to $4-3\\times1.5^2=-2.75.$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create linspace and define function L(w)\n",
    "w_vals = torch.linspace(-2, 2, 1000)\n",
    "L_fn = lambda w: 4 * w - w ** 3\n",
    "\n",
    "# 2. Plot L(w)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.plot(w_vals, L_fn(w_vals))\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('L')\n",
    "ax.axhline(y = 0, color = 'white')\n",
    "ax.axvline(x = 0, color = 'white')\n",
    "\n",
    "# Mark L(1)\n",
    "ax.set_title('L(w) = 4w-w^3 at w=1.5')\n",
    "ax.scatter(1.5, L_fn(torch.tensor(1.5)), c = 'red')\n",
    "ax.scatter(1.5, 0, c='red')\n",
    "plt.show()\n",
    "\n",
    "# 3. Compute gradient of L at w = 1.5\n",
    "w = ?\n",
    "L = ?\n",
    "L.?  # Compute gradient a.k.a. sensitivity\n",
    "print('The gradient of L w.r.t. w at w = 1.5 is %f' % ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Function with multiple inputs.\n",
    "\n",
    "Example: calculate the sensitivity of $L(w_1,w_2) = w_1+w_2^2$ w.r.t. the inputs $w_1, w_2$ at $w_1=1, w_2=2.$\n",
    "\n",
    "Setting $\\mathbf{w} = \\begin{bmatrix}w_1\\\\w_2\\end{bmatrix},$ sensitivity $\\nabla_\\mathbf{w}L= \\begin{bmatrix}\\nabla_{w_1}(w_1+w_2^2)\\\\\\nabla_{w_2}(w_1+w_2^2)\\end{bmatrix} = \\begin{bmatrix}\\nabla_{w_1}(w_1)+\\nabla_{w_1}(w_2^2)\\\\\\nabla_{w_2}(w_1)+\\nabla_{w_2}(w_2^2)\\end{bmatrix} =\\begin{bmatrix}1+0\\\\0+2w_2\\end{bmatrix}=\\begin{bmatrix}1\\\\2w_2\\end{bmatrix},$\n",
    "\n",
    " which at $w_1=1,w_2=2$ is equal to $\\begin{bmatrix}1\\\\4\\end{bmatrix}.$\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables with gradient tracking\n",
    "w1 = ?\n",
    "w2 = ?\n",
    "\n",
    "# Compute the function\n",
    "L = ?\n",
    "\n",
    "# Compute gradients\n",
    "?\n",
    "\n",
    "# Print gradients\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the previous example, we could also calculate the sensitivity w.r.t. all the variables in the vector in one shot.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables as a vector with gradient tracking\n",
    "w = torch.tensor([1.0, 2.0], requires_grad = True)\n",
    "\n",
    "# Compute the function\n",
    "L = w[0] + w[1]**2\n",
    "\n",
    "# Compute gradients\n",
    "L.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Another example of using a tensor variable as input where we calculate the sensitivity w.r.t. all the variables in the tensor:.\n",
    "\n",
    "Consider calculating the sensitivity of $L(\\mathbf{w}) = \\lVert \\mathbf{w}\\rVert^2$ for an $8$-vector $w$ at $\\tiny w=\\begin{bmatrix}0.1\\\\0\\\\0.1\\\\0\\\\0.1\\\\0\\\\0.1\\\\0\\end{bmatrix}.$\n",
    "\n",
    "We know that $\\nabla_\\mathbf{w}\\left(\\lVert\\mathbf{w}\\rVert^2\\right)=2\\mathbf{w}$ which evaluated at $\\tiny w=\\begin{bmatrix}0.1\\\\0\\\\0.1\\\\0\\\\0.1\\\\0\\\\0.1\\\\0\\end{bmatrix}$ is $\\tiny2\\begin{bmatrix}0.1\\\\0\\\\0.1\\\\0\\\\0.1\\\\0\\\\0.1\\\\0\\end{bmatrix}=\\begin{bmatrix}0.2\\\\0\\\\0.2\\\\0\\\\0.2\\\\0\\\\0.2\\\\0\\end{bmatrix}.$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables as a vector with gradient tracking\n",
    "w = torch.tensor([0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0], requires_grad = True)\n",
    "\n",
    "# Compute the function\n",
    "L = torch.norm(w)**2\n",
    "\n",
    "# Compute gradients\n",
    "L.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Gradient calculation in PyTorch when a\n",
    "\n",
    "- variable is part of the computation graph\n",
    "- variable is marked non-trainable\n",
    "- variable is not used at all\n",
    "- variable is wrapped in tensor arithmetic (becoming a constant)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variable\n",
    "w1 = torch.tensor(1.0, requires_grad = True)\n",
    "\n",
    "# Constant tensor (non-trainable)\n",
    "c1 = torch.tensor(-2.0)\n",
    "\n",
    "# Variable treated as constant (no gradient tracking)\n",
    "w2 = torch.tensor(2.0, requires_grad = False)\n",
    "\n",
    "# Variable + constant â†’ becomes a tensor, no gradient tracking\n",
    "c2 = torch.tensor(10.0, requires_grad = True) + 1.0  # treated as constant now\n",
    "\n",
    "# Unused variable\n",
    "w3 = torch.tensor(0.0, requires_grad = True)\n",
    "\n",
    "# Forward computation\n",
    "L = (w1 + c1)**2 + w2**3 + 4 * c2\n",
    "\n",
    "# Backward\n",
    "L.backward()\n",
    "\n",
    "# Gradients\n",
    "print(w1.grad)   # Used in computation\n",
    "print(w2.grad)   # Not tracked (requires_grad = False)\n",
    "print(w3.grad)   # Unused in L\n",
    "print(c1.grad)   # c1 is a constant (no grad tracking in PyTorch)\n",
    "print(c2.grad)   # c2 is a tensor (not a leaf with grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Example: consider calculating the gradient of $\\mathbf{a}(z) = \\begin{bmatrix}a_1(z)\\\\a_2(z)\\end{bmatrix}  = \\begin{bmatrix}2z\\\\z^4\\end{bmatrix}$ at $z = -1.$\n",
    "\n",
    "The gradient is $\\nabla_z(\\mathbf{a})= \\begin{bmatrix}\\nabla_z(a_1) & \\nabla_z(a_2)\\end{bmatrix}=\\begin{bmatrix}\\nabla_z(2z) & \\nabla_z\\left(z^4\\right)\\end{bmatrix}=\\begin{bmatrix}2&4z^3\\end{bmatrix}.$\n",
    "\n",
    "Note that the sum of the gradients $\\nabla_z(a_1)+\\nabla_z(a_2)=2+4z^3$ for $z=-1$ is returned which is equal to $2+4(-1)^3=-2.$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([-1.0], requires_grad = True)\n",
    "\n",
    "# Forward computation\n",
    "a = ?\n",
    "\n",
    "# Backward\n",
    "?\n",
    "\n",
    "# Gradients\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Applying the gradient descent method with\n",
    "\n",
    "- a maximum number of iterations equal to 1000\n",
    "- a stopping tolerance equal to $10^{-6}$\n",
    "- a learning rate of 0.01\n",
    "\n",
    " to minimize $$L(\\mathbf{w}) = (w_1-2)^2+(w_2+3)^2$$ starting from $\\mathbf{w} = \\begin{bmatrix}w_1\\\\w_2\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights as tensors with gradients\n",
    "w = torch.tensor([0.0, 0.0], requires_grad = True)\n",
    "\n",
    "# Hyperparameters\n",
    "maxiter = 10000\n",
    "tol = 1e-06\n",
    "lr = 1e-02\n",
    "norm_grad = float('inf')\n",
    "\n",
    "k = 0\n",
    "while k < maxiter and norm_grad > tol:\n",
    "    # Zero the gradients\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "\n",
    "    # Define the loss function\n",
    "    L = (w[0] - 2)**2 + (w[1] + 3)**2\n",
    "\n",
    "    # Backpropagate to compute gradients\n",
    "    L.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        #w = w + lr * (-w.grad)\n",
    "        w -= lr * w.grad\n",
    "\n",
    "    # Compute the norm of the gradient\n",
    "    norm_grad = w.grad.norm().item()\n",
    "    k += 1\n",
    "\n",
    "    print(f'Iteration {k}: ||grad|| = {norm_grad}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
