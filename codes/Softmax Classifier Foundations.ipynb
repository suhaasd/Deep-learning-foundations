{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7g7bxFCHxGP"
   },
   "source": [
    "$$\\Large\\boxed{\\text{AME 5202 Deep Learning, Even Semester 2026}}$$\n",
    "\n",
    "$$\\large\\text{Theme}: \\underline{\\text{computational foundations of the softmax classifier}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDK9fC6uiBGE"
   },
   "source": [
    "---\n",
    "\n",
    "Load essential libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20W0d4ruQjE4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfYXkqmLiVLM"
   },
   "source": [
    "---\n",
    "\n",
    "Mount Google Drive folder if running Google Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYzBBBxqiaGa"
   },
   "outputs": [],
   "source": [
    "## Mount Google drive folder if running in Colab\n",
    "if('google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/EvenSem2026MAHE'\n",
    "    DATA_DIR = DIR+'/Data/'\n",
    "else:\n",
    "    DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avVZ6D1ZgEUT"
   },
   "source": [
    "---\n",
    "\n",
    "**The patient data matrix with output labels and initial weight matrix**\n",
    "\n",
    "![patient dataset](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=800)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the patient data matrix as a constant tensor\n",
    "X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n",
    "                  [85, 130, 37.0, 110, 14],\n",
    "                  [68, 110, 38.5, 125, 34],\n",
    "                  [90, 140, 38.0, 130, 26],\n",
    "                  [84, 132, 38.3, 146, 30],\n",
    "                  [78, 128, 37.2, 102, 12]],\n",
    "                  dtype = torch.float64)\n",
    "print(X)\n",
    "\n",
    "# True output labels vector\n",
    "y = np.array(['non-diabetic',\n",
    "              'diabetic',\n",
    "              'non-diabetic',\n",
    "              'pre-diabetic',\n",
    "              'diabetic',\n",
    "              'pre-diabetic'])\n",
    "print(y)\n",
    "\n",
    "W = torch.tensor([[-0.1, 0.5, 0.3],\n",
    "                  [0.9, 0.3, 0.5],\n",
    "                  [-1.5, 0.4, 0.1],\n",
    "                  [0.1, 0.1, -1.0],\n",
    "                  [-1.2, 0.5, -0.8]], dtype = torch.float64,\n",
    "                 requires_grad = True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "One-hot encoding of the true output labels\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following does not work in PyTorch\n",
    "#y = torch.tensor(['non-diabetic', 'diabetic'])\n",
    "\n",
    "# Create a 1D-numpy array of output labels (equivalent to a rank-1 tensor in\n",
    "# PyTorch which itself is equivalent to a vector in pen & paper)\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output = False)\n",
    "Y = torch.tensor(ohe.fit_transform(y.reshape(-1, 1)), dtype = torch.float64)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Loss for each sample can be quantified using the categorical crossentropy (CCE) loss function which is defined as $$\\color{yellow}{-\\log(\\text{predicted probability that the sample belongs its correct class})}$$\n",
    "\n",
    "For example, consider a sample with\n",
    "\n",
    "- true label = [$\\color{yellow}{1}$ 0 0]\n",
    "- predicted probabilities (the softmax-activated scores) = [$\\color{yellow}{0.05}$, 0.99, 0.05]\n",
    "\n",
    "$\\Rightarrow$ categorical crossentropy loss = $-\\log(\\color{yellow}{0.05}).$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([1.0, 0.0, 0.0])\n",
    "a = torch.tensor([0.05, 0.99, 0.05])\n",
    "-torch.log(torch.sum(y*a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The forward propagation without and with standardization of the data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without standardizing the data\n",
    "\n",
    "# Calculate raw scores\n",
    "Z = torch.matmul(X, W)\n",
    "print(f'The raw scores matrix:\\n{Z}')\n",
    "\n",
    "# Calculate the softmax-activated scores matrix\n",
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "A = softmax(Z)\n",
    "print(f'The softmax-activated scores matrix:\\n{A}')\n",
    "print(f'One-hot encoded true output labels:\\n{Y}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Quantify the unhappiness w.r.t. the current set of weights for all the samples\n",
    "print(f'Average training loss = {torch.mean(-torch.log(torch.sum(Y*A, dim = 1)))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "sc = StandardScaler() # create a standard scaler object\n",
    "X_std = torch.tensor(sc.fit_transform(X), dtype = torch.float64)\n",
    "print(f'The standardized data matrix:\\n{X_std}')\n",
    "\n",
    "# Calculate the raw scores using the standardized data matrix\n",
    "# and the weights matrix\n",
    "Z = torch.matmul(X_std, W)\n",
    "print(f'The raw scores matrix:\\n{Z}')\n",
    "\n",
    "# Calculate the softmax-activated scores matrix\n",
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "A = softmax(Z)\n",
    "print(f'The softmax-activated scores matrix:\\n{A}')\n",
    "print(f'One-hot encoded true output labels matrx:\\n{Y}')\n",
    "\n",
    "# Quantify the unhappiness w.r.t. the current set of weights a.k.a. the training loss\n",
    "print(f'Average training loss = {torch.mean(-torch.log(torch.sum(Y*A, dim = 1)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Using PyTorch, calculate the optimal weights for the patient data matrix\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patients data matrix\n",
    "X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n",
    "                 [85, 130, 37.0, 110, 14],\n",
    "                 [68, 110, 38.5, 125, 34],\n",
    "                 [90, 140, 38.0, 130, 26],\n",
    "                 [84, 132, 38.3, 146, 30],\n",
    "                 [78, 128, 37.2, 102, 12]], dtype = torch.float64)\n",
    "#print(f'Patient data matrix X:\\n {X}') #f-string in Python\n",
    "\n",
    "# Initial Weights matrix (trainable tensor)\n",
    "W = torch.tensor([[0.9, 0.5, 0.3],\n",
    "                  [0.9, 0.3, 0.5],\n",
    "                  [-1.5, 0.4, 0.1],\n",
    "                  [0.1, 0.1, -1.0],\n",
    "                  [-1.2, 0.5, -0.8]], dtype = torch.float64,\n",
    "                 requires_grad = True)\n",
    "#print(f'Weights matrix:\\n {W}')\n",
    "\n",
    "# Create a 1D-numpy array of output labels (equivalent to a rank-1 tensor in\n",
    "# PyTorch which itself is equivalent to a vector in pen & paper)\n",
    "y = np.array(['non-diabetic',\n",
    "              'diabetic',\n",
    "              'non-diabetic',\n",
    "              'pre-diabetic',\n",
    "              'diabetic',\n",
    "              'pre-diabetic'])\n",
    "# Creating a one-hot encoder object\n",
    "ohe = OneHotEncoder(sparse_output = False)\n",
    "# Create the one-hot encoded true output labels matrix\n",
    "Y = torch.tensor(ohe.fit_transform(y.reshape(-1, 1)), dtype = torch.float64)\n",
    "#print(f'One-hot encoded output labels matrix:\\n {Y}')\n",
    "\n",
    "# Standardize the data\n",
    "sc = StandardScaler() # create a standard scaler object\n",
    "X_std = torch.tensor(sc.fit_transform(X), dtype = torch.float64)\n",
    "#print(f'The standardized data matrix:\\n{X_std}')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam([W], lr = 1e-02)\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(W):\n",
    "  # Raw scores\n",
    "  Z = torch.matmul(X_std, W)\n",
    "\n",
    "  # Softmax-activated scores\n",
    "  softmax = torch.nn.Softmax(dim = 1)\n",
    "  A = softmax(Z)\n",
    "\n",
    "  # Calculate the average training loss\n",
    "  L = torch.mean(-torch.log(torch.sum(Y*A, dim = 1)))\n",
    "  return L\n",
    "\n",
    "# Optimization loop\n",
    "num_epochs = 1000\n",
    "loss_train = np.empty(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "  # Zero out the gradients\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Forward propagation (loss calculation)\n",
    "  loss = loss_fn(W)\n",
    "\n",
    "  # Backward propagation and optimization\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # Print the loss every epoch\n",
    "  loss_train[epoch] = loss.item()\n",
    "  print(f'Epoch {epoch}, loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Plot training loss curve\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot train loss as a function of epoch:\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "fig.tight_layout(pad = 4.0)\n",
    "ax.plot(loss_train, 'b')\n",
    "ax.set_xlabel('Epoch', fontsize = 12)\n",
    "ax.set_ylabel('Loss value', fontsize = 12)\n",
    "ax.set_title('Training Loss vs. Epoch', fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the optimized Weights matrix\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "How good are the weights when applied to the training data?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  # Raw scores \n",
    "  Z = torch.matmul(X_std, W)\n",
    "\n",
    "  # Softmax-activated scores \n",
    "  softmax = torch.nn.Softmax(dim = 1)\n",
    "  A = softmax(Z)\n",
    "\n",
    "# Get predicted labels \n",
    "print(f'Predicted labels for training data: {A.argmax(dim = 1)}')\n",
    "\n",
    "# True output labels\n",
    "print(f'True output labels: {Y.argmax(dim = 1)}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
