{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7g7bxFCHxGP"
   },
   "source": [
    "$$\\Large\\boxed{\\text{AME 5202 Deep Learning, Even Semester 2026}}$$\n",
    "\n",
    "$$\\large\\text{Theme}: \\underline{\\text{computational foundations of the softmax classifier}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDK9fC6uiBGE"
   },
   "source": [
    "---\n",
    "\n",
    "Load essential libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "20W0d4ruQjE4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfYXkqmLiVLM"
   },
   "source": [
    "---\n",
    "\n",
    "Mount Google Drive folder if running Google Colab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VYzBBBxqiaGa"
   },
   "outputs": [],
   "source": [
    "## Mount Google drive folder if running in Colab\n",
    "if('google.colab' in sys.modules):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount = True)\n",
    "    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/EvenSem2026MAHE'\n",
    "    DATA_DIR = DIR+'/Data/'\n",
    "else:\n",
    "    DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avVZ6D1ZgEUT"
   },
   "source": [
    "---\n",
    "\n",
    "**The patient data matrix with output labels and initial weight matrix**\n",
    "\n",
    "![patient dataset](https://1drv.ms/i/s!AjTcbXuSD3I3hspfrgklysOtJMOjaA?embed=1&width=800)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n",
      "        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n",
      "        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n",
      "        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n",
      "        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n",
      "        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]],\n",
      "       dtype=torch.float64)\n",
      "['non-diabetic' 'diabetic' 'non-diabetic' 'pre-diabetic' 'diabetic'\n",
      " 'pre-diabetic']\n",
      "tensor([[-0.1000,  0.5000,  0.3000],\n",
      "        [ 0.9000,  0.3000,  0.5000],\n",
      "        [-1.5000,  0.4000,  0.1000],\n",
      "        [ 0.1000,  0.1000, -1.0000],\n",
      "        [-1.2000,  0.5000, -0.8000]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Create the patient data matrix as a constant tensor\n",
    "X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n",
    "                  [85, 130, 37.0, 110, 14],\n",
    "                  [68, 110, 38.5, 125, 34],\n",
    "                  [90, 140, 38.0, 130, 26],\n",
    "                  [84, 132, 38.3, 146, 30],\n",
    "                  [78, 128, 37.2, 102, 12]],\n",
    "                  dtype = torch.float64)\n",
    "print(X)\n",
    "\n",
    "# True output labels vector\n",
    "y = np.array(['non-diabetic',\n",
    "              'diabetic',\n",
    "              'non-diabetic',\n",
    "              'pre-diabetic',\n",
    "              'diabetic',\n",
    "              'pre-diabetic'])\n",
    "print(y)\n",
    "\n",
    "W = torch.tensor([[-0.1, 0.5, 0.3],\n",
    "                  [0.9, 0.3, 0.5],\n",
    "                  [-1.5, 0.4, 0.1],\n",
    "                  [0.1, 0.1, -1.0],\n",
    "                  [-1.2, 0.5, -0.8]], dtype = torch.float64,\n",
    "                 requires_grad = True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "One-hot encoding of the true output labels\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# The following does not work in PyTorch\n",
    "#y = torch.tensor(['non-diabetic', 'diabetic'])\n",
    "\n",
    "# Create a 1D-numpy array of output labels (equivalent to a rank-1 tensor in\n",
    "# PyTorch which itself is equivalent to a vector in pen & paper)\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output = False)\n",
    "Y = ohe.fit_transform(y.reshape(-1, 1))\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Loss for each sample can be quantified using the categorical crossentropy (CCE) loss function which is defined as $$\\color{yellow}{-\\log(\\text{predicted probability that the sample belongs its correct class})}$$\n",
    "\n",
    "For example, consider a sample with\n",
    "\n",
    "- true label = [$\\color{yellow}{1}$ 0 0]\n",
    "- predicted probabilities = [$\\color{yellow}{0.05}$, 0.99, 0.05]\n",
    "\n",
    "$\\Rightarrow$ categorical crossentropy loss = $-\\log(\\color{yellow}{0.05}).$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9957)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([1.0, 0.0, 0.0])\n",
    "a = torch.tensor([0.05, 0.99, 0.05])\n",
    "-torch.log(torch.sum(y*a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The forward propagation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 72.0000, 120.0000,  37.3000, 104.0000,  32.5000],\n",
      "        [ 85.0000, 130.0000,  37.0000, 110.0000,  14.0000],\n",
      "        [ 68.0000, 110.0000,  38.5000, 125.0000,  34.0000],\n",
      "        [ 90.0000, 140.0000,  38.0000, 130.0000,  26.0000],\n",
      "        [ 84.0000, 132.0000,  38.3000, 146.0000,  30.0000],\n",
      "        [ 78.0000, 128.0000,  37.2000, 102.0000,  12.0000]],\n",
      "       dtype=torch.float64)\n",
      "The standardized data matrix:\n",
      "[[-0.979883   -0.70186241 -0.72380201 -0.98707429  0.89204786]\n",
      " [ 0.71858087  0.3509312  -1.24493946 -0.60498101 -1.2373567 ]\n",
      " [-1.50248727 -1.75465602  1.36074779  0.35025217  1.06470228]\n",
      " [ 1.3718362   1.40372481  0.49218537  0.66866323  0.14387869]\n",
      " [ 0.5879298   0.56148993  1.01332282  1.68757862  0.60429048]\n",
      " [-0.1959766   0.14037248 -0.8975145  -1.11443871 -1.4675626 ]]\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data\n",
    "sc = StandardScaler() # create a standard scaler object\n",
    "print(X)\n",
    "X_std = sc.fit_transform(X)\n",
    "print(f'The standardized data matrix:\\n{X_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "sc = StandardScaler() # create a standard scaler object\n",
    "X_std = ?\n",
    "print(f'The standardized data matrix:\\n{X_std}')\n",
    "\n",
    "# The one-hot encoded true output labels matrix\n",
    "ohe = OneHotEncoder(sparse_output = False)\n",
    "Y = ?\n",
    "print(f'One-hot encoded true output labels matrx:\\n{Y}')\n",
    "\n",
    "# Calculate the raw scores using the standardized data matrix\n",
    "# and the weights matrix\n",
    "print(f'The weights matrix:\\n{W}')\n",
    "Z = ?\n",
    "print(f'The raw scores matrix:\\n{Z}')\n",
    "\n",
    "# Calculate the softmax-activated scores matrix\n",
    "softmax = ?\n",
    "A = ?\n",
    "print(f'The softmax-activated scores matrix:\\n{A}')\n",
    "\n",
    "# Quantify the unhappiness w.r.t. the current set of weights\n",
    "print(f'Loss = {?}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Using PyTorch, calculate the optimal weights for the patient data matrix\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patients data matrix\n",
    "X = torch.tensor([[72, 120, 37.3, 104, 32.5],\n",
    "                 [85, 130, 37.0, 110, 14],\n",
    "                 [68, 110, 38.5, 125, 34],\n",
    "                 [90, 140, 38.0, 130, 26],\n",
    "                 [84, 132, 38.3, 146, 30],\n",
    "                 [78, 128, 37.2, 102, 12]], dtype = torch.float64)\n",
    "#print(f'Patient data matrix X:\\n {X}') #f-string in Python\n",
    "\n",
    "# Initial Weights matrix (trainable tensor)\n",
    "W = torch.tensor([[-0.1, 0.5, 0.3],\n",
    "                  [0.9, 0.3, 0.5],\n",
    "                  [-1.5, 0.4, 0.1],\n",
    "                  [0.1, 0.1, -1.0],\n",
    "                  [-1.2, 0.5, -0.8]], dtype = torch.float64,\n",
    "                 requires_grad = True)\n",
    "#print(f'Weights matrix:\\n {W}')\n",
    "\n",
    "# Create a 1D-numpy array of output labels (equivalent to a rank-1 tensor in\n",
    "# PyTorch which itself is equivalent to a vector in pen & paper)\n",
    "y = np.array(['non-diabetic',\n",
    "              'diabetic',\n",
    "              'non-diabetic',\n",
    "              'pre-diabetic',\n",
    "              'diabetic',\n",
    "              'pre-diabetic'])\n",
    "# Creating a one-hot encoder object\n",
    "ohe = OneHotEncoder(sparse_output = False)\n",
    "# Create the one-hot encoded true output labels matrix\n",
    "Y = torch.tensor(ohe.fit_transform(y.reshape(-1, 1)), dtype = torch.float64)\n",
    "#print(f'One-hot encoded output labels matrix:\\n {Y}')\n",
    "\n",
    "# Standardize the data\n",
    "sc = StandardScaler() # create a standard scaler object\n",
    "X_std = torch.tensor(sc.fit_transform(X), dtype = torch.float64)\n",
    "#print(f'The standardized data matrix:\\n{X_std}')\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = ?\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(W):\n",
    "  # Raw scores\n",
    "  Z = ?\n",
    "\n",
    "  # Softmax-activated scores\n",
    "  softmax = ?\n",
    "  A = ?\n",
    "\n",
    "  # Calculate the average training loss\n",
    "  L = ?\n",
    "  return L\n",
    "\n",
    "# Optimization loop\n",
    "num_epochs = ?\n",
    "loss_train = ?\n",
    "for epoch in range(num_epochs):\n",
    "  # Zero out the gradients\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Forward propagation (loss calculation)\n",
    "  loss = ?\n",
    "\n",
    "  # Backward propagation and optimization\n",
    "  ?\n",
    "\n",
    "  # Print the loss every epoch\n",
    "  loss_train[epoch] = loss.item()\n",
    "  print(f'Epoch {epoch}, loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Plot loss curve\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot train loss as a function of epoch:\n",
    "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
    "fig.tight_layout(pad = 4.0)\n",
    "ax.plot(loss_train, 'b')\n",
    "ax.set_xlabel('Epoch', fontsize = 12)\n",
    "ax.set_ylabel('Loss value', fontsize = 12)\n",
    "ax.legend()\n",
    "ax.set_title('Loss vs. Epoch', fontsize = 14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the optimized Weights matrix\n",
    "print(W)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
